---
title: "Feature Selection and Modeling by Model"
author: "Tanu Kajla"
date: "January 25, 2020"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r Check installed packages, echo = FALSE, warning=FALSE, message=FALSE}
# Creating a vector of packages used within
packages <- c(
  'arm',
  'boot',
  'Boruta',
  'car',
  'caret',
  'caTools',
  'data.table',
  'DMwR',
  'dplyr',
  'e1071',
  'Information',
  'klaR',
  #'lares',
  'leaps',
  'lubridate',
  'magrittr',
  'MASS',
  'mctest',
  'mlbench',
  'MLeval',
  'pastecs',
  'PerformanceAnalytics',
  'pROC',
  'proxy',
  'pscl',
  'psych',
  'ranger',
  'ROCR',
  'Rtsne',
  'scales',
  'splitstackshape',
  'tidyselect',
  'tidyverse',
  'varrank',
  'VIM',
  'zoo',
  'corrplot',
  'glmnet',
  'doParallel',
  'foreach',
  'broom',
  'DT',
  'AppliedPredictiveModeling'
)

# Checking for package installations on the system and installing if not found
if (length(setdiff(packages, rownames(installed.packages()))) > 0) {
  install.packages(setdiff(packages, rownames(installed.packages())))  
}

# Including the packages for use
for(package in packages){
  library(package, character.only = TRUE)
}

registerDoParallel(cores = 4)

```

### Data Processing

```{r Model Data by Day}

########### Data Pre-Processing  ###########
# Read dataset from RDS
model.data <- readRDS("KDEN_FINAL.RDS")

#Running the Code to Group the Data by Day. THis removes Rtime, Arrivals, Departures
model.data.day <- model.data %>%
  group_by(`AIRPORT ID`,DATE) %>%
  summarise(
    FLIGHTCOUNT = max(`TOTAL`),
    STRIKECOUNT = sum(STRIKE),
    #Sum of Strikes
    STRIKE = sum(STRIKE),
    #Will be Converted to Binary Strikes
    BIRDCOUNT = mean(BIRDCOUNT),
    TEMP = mean(TEMP, na.rm = TRUE),
    SLP = mean(SLP, na.rm = TRUE),
    `WIND ANGLE` = mean(`WIND ANGLE`, na.rm = TRUE),
    `WIND SPEED` = mean(`WIND SPEED`, na.rm = TRUE),
    `COVER` = mean(`COVER`, na.rm = TRUE),
    CLOUDH = mean(CLOUDH, na.rm = T),
    COVERH = mean(COVERH, na.rm = T),
    PRECIP = mean(PRECIP, na.rm = T),
    VIS = mean(VIS, na.rm = T)
  ) %>%
  mutate(STRIKE = ifelse(STRIKE > 0, 1, 0))

#Converting the new day data into a data frame 
model.data.day<-as.data.frame(model.data.day)
model.data.day$STRIKE<-as.factor(model.data.day$STRIKE)

# Add Year, Month, Day and Weekday fields to  the dataset as factors
# Bind new cols to the original dataset
model.data.day <- cbind(
  model.data.day,
  data.frame(
    YEAR = as.factor(format(model.data.day$DATE, format = "%Y")),
    MONTH = as.factor(format(model.data.day$DATE, format = "%m")),
    DAY = as.factor(format(model.data.day$DATE, format = "%d")),
    WEEKOFYEAR = as.factor(format(model.data.day$DATE, format = "%V"))
  )
)

# Remove AirportId and Date columns
model.data.day <- subset(model.data.day, select = -c(`AIRPORT ID`, DATE))

# Rename Windangle and windspeed
model.data.day <-
  model.data.day %>% dplyr::rename(WANGLE = `WIND ANGLE`,
                                   WSPEED = `WIND SPEED`)

# Because of memory constraints considering only 2 years data
model.data.day <-
  model.data.day %>% filter(!model.data.day$YEAR %in% c(2013, 2019))

model.data.day <- model.data.day %>%
  mutate(STRIKE = ifelse(STRIKE == 0, "NO", "YES"))

rm(model.data)
```


### VIF and Correlation Plots


```{r correlation data}

#External Plot for Corr Plot 
source("http://www.sthda.com/upload/rquery_cormat.r")

corr.data <-
  model.data.day %>% dplyr::select(
    STRIKECOUNT,
    # WANGLE,
    WSPEED,
    # TEMP,
    # CLOUDH,
    COVER,
    # COVERH,
    BIRDCOUNT,
    FLIGHTCOUNT,
    # SLP,
    # VIS,
    PRECIP
  )
corr.list <- rquery.cormat(corr.data, type = "flatten", graph = FALSE)
cormat <- rquery.cormat(corr.data, graphType = "heatmap")
rquery.cormat(corr.data, type = "full")
corr <- corr.list$r

```

```{r }

#VIF Function
VIF <-function(linear.model,no.intercept = FALSE, all.diagnostics = FALSE,plot = FALSE) {
    if (no.intercept == FALSE)
      design.matrix <- model.matrix(linear.model)[, -1]
    if (no.intercept == TRUE)
      design.matrix <- model.matrix(linear.model)
    if (plot == TRUE)
      mc.plot(design.matrix, linear.model$model[1])
    if (all.diagnostics == FALSE)
      output <-
        imcdiag(design.matrix,
                linear.model$model[1],
                method = 'VIF')$idiags[, 1]
    if (all.diagnostics == TRUE)
      output <-
        imcdiag(design.matrix, linear.model$model[1])
    output
}
```


```{r VIF}
set.seed(123)
strikes.sample <-
  model.data.day[, c(
    "STRIKECOUNT",
    "WANGLE",
    "WSPEED",
    "TEMP",
    "COVER",
    "BIRDCOUNT",
    "FLIGHTCOUNT",
    "PRECIP", 
    "CLOUDH",
    "COVERH",
    "VIS",
    "SLP"
  )]

strikes.sample$STRIKECOUNT <- as.numeric(strikes.sample$STRIKECOUNT)

# strikes.sample[, 2:12] <- scale(strikes.sample[, 2:12])

fit <- glm(STRIKECOUNT ~ ., data = strikes.sample)
VIF(fit, all.diagnostics = TRUE, plot = TRUE)

#This function is a simple port of vif from the car package. The VIF of a predictor is a measure for how easily it is predicted from a linear regression using the other predictors. Taking the square root of the VIF tells you how much larger the standard error of the estimated coefficient is respect to the case when that predictor is independent of the other predictors.

```


```{r Regression Formulas}

reg.formula.corr <- STRIKECOUNT ~ WSPEED + COVER + BIRDCOUNT + FLIGHTCOUNT + PRECIP + MONTH + YEAR

reg.formula.vif <- STRIKECOUNT ~ WANGLE + COVER + MONTH + YEAR + WEEKOFYEAR

reg.formula.best <- STRIKECOUNT ~ WSPEED + WANGLE + COVER + BIRDCOUNT + FLIGHTCOUNT + MONTH + YEAR + STRIKE + DAY + TEMP

reg <- c(reg.formula.corr, reg.formula.vif, reg.formula.best)

reg

```

```{r Classification Formulas}

cl.formula.corr <- c('WSPEED', 'COVER', 'BIRDCOUNT', 'FLIGHTCOUNT', 'PRECIP', 'MONTH', 'YEAR')

cl.formula.vif <- STRIKE ~  WANGLE + COVER + MONTH + YEAR + WEEKOFYEAR

cl.formula.best <- STRIKE ~ WSPEED + WANGLE + COVER + BIRDCOUNT + FLIGHTCOUNT + MONTH + YEAR + STRIKE + DAY + TEMP

cl <- c(cl.formula.corr, cl.formula.vif, cl.formula.best)

cl

```


```{r}
########### Create the Training and Test Data Set Data Sets for Regression Models###########
# Create the training and test datasets
set.seed(500)

# Step 1: Get row numbers for the training data
trainRowNumbers <-
  createDataPartition(model.data.day$STRIKECOUNT, p = 0.8, list = FALSE)

# Step 2: Create the training  dataset
train.data <- model.data.day[trainRowNumbers, ]

# Step 3: Create the test dataset
test.data <- model.data.day[-trainRowNumbers, ]
```



```{r Functions & Parameters}

########### Parameters ###########

train_control <- trainControl(method="cv", number=5)

#all variables excluding the strikes factor (highly correlated, no value)
#total variables
regression<-STRIKECOUNT~
  FLIGHTCOUNT+
  #STRIKE+
  BIRDCOUNT+
  TEMP+
  SLP+
  WANGLE+
  WSPEED+
  COVER+
  CLOUDH+
  COVERH+
  PRECIP+
  VIS+
  MONTH+
  #DAY+
  WEEKOFYEAR

#selected variables
regression1<-STRIKECOUNT~
  FLIGHTCOUNT+
  #STRIKE+
  BIRDCOUNT+
  TEMP+
  #SLP+
  WANGLE+
  WSPEED+
  #COVER+
  CLOUDH+
  COVERH+
  #PRECIP+
  VIS+
  #YEAR+
  MONTH+
  DAY+
  WEEKOFYEAR

#removal of time variables
regression2<-STRIKECOUNT~
  FLIGHTCOUNT+
  #STRIKE+
  BIRDCOUNT+
  TEMP+
  SLP+
  WANGLE+
  WSPEED+
  COVER+
  CLOUDH+
  COVERH+
  PRECIP+
  VIS
  #MONTH+
  #DAY+
  #WEEKOFYEAR



```



#Count Modeling

```{r Validate Function}
validateAndPrintResultC <- function(model, test.data) {
  # Summarise Results
  print(model)
  summary(model)
  model$finalModel

  # Predict on testData
  predicted.resp <- round(predict(model, test.data))
  
  # head(predicted.resp)
  compare<- test.data%>% 
  dplyr::select("YEAR","MONTH","DAY","STRIKECOUNT")
  
  compare<-cbind(compare,predicted.resp)
  compare$accuracy<- ifelse(compare$STRIKECOUNT == compare$predicted.resp,TRUE,FALSE)
  acc<- filter(compare, accuracy == TRUE, predicted.resp != 0) 
  false<- filter(compare, accuracy == FALSE)
  print(table(compare$accuracy))
  print(acc)
  print(false)

}
```

```{r Stepwise Feature Selection Model}
########### Stepwise Feature Selection (fully fitted)
#Run a Regular Stepwise Linar Regression on the Entire DataSet --- TAKES A LONG TIME WITH TIME VARIABLES
for (reg.formula in reg) {
  glmStepAIC <- train(reg.formula,
  data = model.data.day,
  method = "glmStepAIC",
  trControl = train_control)
  #Print the Results.. This is totally overfitted because it was run on the entire data set to get feature selection
  validateAndPrintResultC(glmStepAIC, test.data)
}



########### RANDOM AIC RESULTS ########### 

# 729 samples
#  12 predictor
# 
# No pre-processing
# Resampling: Cross-Validated (5 fold) 
# Summary of sample sizes: 584, 583, 583, 583, 583 
# Resampling results:
# 
#   RMSE       Rsquared   MAE      
#   0.3121226  0.7730515  0.1370295
# 
# 
# FALSE  TRUE 
#    10   135 

#Altnerate modelbase
#modelbase<-glm(STRIKECOUNT~ FLIGHTCOUNT+ STRIKE+ BIRDCOUNT+ TEMP+ SLP+ WANGLE+ WSPEED+ COVER+ CLOUDH+ COVERH+ PRECIP+ VIS+ WEEKOFYEAR, data = model.data.day)
#alternate feature selection 
#step <- stepAIC(modelbase)
#step$anova # display results

# define training control

# Step:  AIC=329.67
# .outcome ~ STRIKE + TEMP + WANGLE + WSPEED + COVER + CLOUDH + 
#     PRECIP

# Step:  AIC=377.81
# .outcome ~ STRIKE1 + TEMP + WANGLE + COVER + VIS
# 
#           Df Deviance     AIC
# <none>         70.306  377.81
# - VIS      1   70.568  378.53
# - COVER    1   70.606  378.92
# - WANGLE   1   70.650  379.37
# - TEMP     1   70.668  379.56
# - STRIKE1  1  291.892 1413.57
# Generalized Linear Model with Stepwise Feature Selection 

#After runing all the variables

# Step:  AIC=1361.23
# .outcome ~ WSPEED + VIS + MONTH08 + MONTH09 + MONTH10 + DAY03 + 
#     DAY07 + DAY29 + WEEKOFYEAR19 + WEEKOFYEAR20 + WEEKOFYEAR21 + 
#     WEEKOFYEAR22 + WEEKOFYEAR23 + WEEKOFYEAR24 + WEEKOFYEAR25 + WEEKOFYEAR26 + 
#     WEEKOFYEAR27 + WEEKOFYEAR29 + WEEKOFYEAR30 + WEEKOFYEAR31 + WEEKOFYEAR32 + 
#     WEEKOFYEAR33 + WEEKOFYEAR34 + WEEKOFYEAR35 + WEEKOFYEAR37 + WEEKOFYEAR40

#             Df Deviance    AIC
# <none>           801.92 3691.3
# - BIRDCOUNT  1   802.88 3691.5
# - FLIGHTCOUNT      1   803.52 3693.0
# - VIS        1   804.20 3694.5
# - SLP        1   804.28 3694.7
# - COVERH     1   806.00 3698.6
# - TEMP       1   815.75 3720.5
# Generalized Linear Model with Stepwise Feature Selection 
# 
# 1822 samples
#   11 predictor


```

```{r Regression Model}
########### General Model ########### 
modelglm <- train(regression1,
                  data=train.data,
                  method="glm" ,
                  trControl = train_control)

validateAndPrintResultC(modelglm,test.data)

plot(caret::varImp(modelglm))

########### Results ########### 

# 584 samples
#  11 predictor
# 
# No pre-processing
# Resampling: Cross-Validated (5 fold) 
# Summary of sample sizes: 468, 468, 467, 467, 466 
# Resampling results:
# 
#   RMSE       Rsquared    MAE      
#   0.6766016  0.03640392  0.4935009
# 
# prediction from a rank-deficient fit may be misleading
# FALSE  TRUE 
#    54    91 

# 8 Accurate predicted Flights

```


```{r Negative Binomial}
#Negative Binomial
modelglm.nb <- glm.nb(regression1,
                  data=train.data, link = "sqrt",
                  method = "glm.fit")

validateAndPrintResultC(modelglm.nb,test.data)

plot(modelglm.nb)

########### Results ########### 
# Degrees of Freedom: 1458 Total (i.e. Null);  1357 Residual
# Null Deviance:	    1318 
# Residual Deviance: 1109 	AIC: 2295
# 
# FALSE  TRUE 
#   207   156 
########### Failed Code ########### 

## want to try the NEGBINOMAIL with CAROT -- DOESN"T WORK
# train_control <- trainControl(method="repeatedcv", number=3, repeats=2)
# 
# modelglm.nb <- train(STRIKECOUNT~
#                        FLIGHTCOUNT+
#                         #STRIKE+
#                         BIRDCOUNT+
#                         TEMP+
#                         #SLP+
#                         WANGLE+
#                         WSPEED+
#                         #COVER+
#                         CLOUDH+
#                         COVERH+
#                         #PRECIP+
#                         VIS,
#                         #YEAR+
#                         #MONTH+
#                         #DAY+
#                         #WEEKOFYEAR,
#                   data=train.data,
#                   method="glm.nb" ,
#                   trControl = trainControl(method = "boot"),
#                   link="sqrt",
#                   trace =TRUE,
#                   maxit = 10)
# validateAndPrintResultC(modelglm.nb,test.data)

```


```{r Poisson Model }
##Poisson
modelPos <- glm(regression1, 
               data = train.data, 
               family= poisson(link = "sqrt"))

validateAndPrintResultC(modelPos,test.data)

plot(modelPos)

########### Results ########### 

#Updating the link
# Degrees of Freedom: 1458 Total (i.e. Null);  1357 Residual
# Null Deviance:	    1464 
# Residual Deviance: 1236 	AIC: 2300
# 
# FALSE  TRUE 
#   208   155 



#The model is currently predicting negative strikes.. how do i control for this? 

# Degrees of Freedom: 583 Total (i.e. Null);  483 Residual
# Null Deviance:	    550.9 
# Residual Deviance: 383.6 	AIC: 920.8
# 
# FALSE  TRUE 
#   129    16 

```

```{r Quasi Poisson}
##QuasiPosson
modelQPos<-glm(regression1, 
              data = train.data, quasipoisson)

validateAndPrintResultC(modelQPos,test.data)

plot(modelQPos)

#Looks Promising 

```


```{r Information Gain}
model.hurdle<- pscl::hurdle(regression3, data = train.data, dist = "poisson", zero.dist = "binomial")
validateAndPrintResultC(model.hurdle,test.data)

regression3<-STRIKECOUNT~
  FLIGHTCOUNT+
  #STRIKE+
  #BIRDCOUNT
  #TEMP+
  #SLP+
  #WANGLE+
  #WSPEED+
  #COVER+
  #CLOUDH+
  #COVERH+
  #PRECIP+
  #VIS
  #YEAR+
  #MONTH+
  #DAY+
  #WEEKOFYEAR

#Cant run the hurdle since the variables are highly correlated with one another
  
```


```{r ols}
ols <-lm(regression1, data = train.data)
validateAndPrintResultC(ols,test.data)
plot(ols)

```


```{r Fixed Effects-- DOESN't WORK}
regression.time<-STRIKECOUNT~
   FLIGHTCOUNT+
  # #STRIKE+
   BIRDCOUNT+
  # TEMP+
  # #SLP+
  # WANGLE+
   WSPEED+
  # #COVER+
  # CLOUDH+
  # COVERH+
  # #PRECIP+
  # VIS+
  #YEAR+
  MONTH+
  factor(DAY)+
  factor(WEEKOFYEAR)

fixed.dum <-lm(regression.time, data = train.data)
validateAndPrintResultC(fixed.dum,test.data)

summary(fixed.dum)
plot(fixed.dum)
fixed.time<- plm::plm(regression.time, 
                      data = train.data, 
                      model = "within")

summary(fixed.time)
random.time <- plm::plm(regression.time, 
                  data=train.data, 
                  model="random")
# effect = c("individual",
#   "time", "twoways", "nested")
# model = c("within", "random", "ht",
#   "between", "pooling", "fd")

summary(fixed)
#doesn't explain anything and has a very low r^2

```

```{r}
# To decide between fixed or random effects you can run a Hausman test where the null hypothesis is that the preferred model is random effects vs. the alternative the fixed effects (see Green, 2008, chapter 9). It basically tests whether the unique errors are correlated with the regressors, the null hypothesis is they are not. If the p-value is significant (for example <0.05) then use fixed effects, if not use random effects.

#phtest(fixed, random)

```

#Classification Modeling

```{r}
########### Functions ###########


runModel <- function(train.data, method.name) {
  caret::train(
    train.data[, c("WSPEED",
                   "TEMP",
                   "BIRDCOUNT",
                   "FLIGHTCOUNT",
                   "COVER",
                   "WEEKOFYEAR",
                   "MONTH")],
    train.data[, c("STRIKE")],
    data = train.data,
    tuneLength = 5,
    method = method.name,
    metric = 'ROC',
    trControl = tc,
    preProcess = c("center", "scale")
  )
}

```


```{r}
########### Create the Training and Test Data Set Data Sets for Classification Models###########
# Create the training and test datasets
set.seed(500)

# Step 1: Get row numbers for the training data
trainRowNumbers <-
  createDataPartition(model.data.day$STRIKE, p = 0.8, list = FALSE)

# Step 2: Create the training  dataset
train.data <- model.data.day[trainRowNumbers, ]

# Step 3: Create the test dataset
test.data <- model.data.day[-trainRowNumbers, ]
```

```{r}
validateAndPrintResult <- function(model, test.data) {
  # Summarise Results
  print(model)
  
  ## run MLeval
  res <- evalm(model)
  
  # Predict on testData
  predicted.resp <- predict(model, test.data)
  head(predicted.resp)
  
  # Compute the confusion matrix
  confusionMatrix(
    reference = as.factor(test.data$STRIKE),
    data = predicted.resp,
    mode = 'everything',
    positive = 'YES'
  )
}
```




```{r random sample of year}

# years.list <- c(2014, 2015, 2016, 2017, 2018)
# 
# for (year in years.list) {
#   set.seed(42)  # good idea to set the random seed for reproducibility
#   temp.data <- model.data %>% filter(model.data$YEAR == year)
#   temp.0.data <- temp.data %>% filter(temp.data$STRIKE == 0)
#   temp.1.data <- temp.data %>% filter(temp.data$STRIKE == 1)
#   sample.0.data <- stratified(temp.0.data, c('STRIKE'), 0.02)
#   sample.1.data <- stratified(temp.1.data, c('STRIKE'), 0.4)
#   sample.data <- rbind(sample.0.data, sample.1.data)
#   if (year == 2014) {
#     strikes.data <- sample.data
#   }
#   else{
#     strikes.data <- bind_rows(strikes.data, sample.data)
#   }
# }
# 
# rm(temp.0.data, temp.1.data, temp.data, sample.0.data, sample.1.data, sample.data)

#creating model.data.day to strikes.data

strikes.data.day <- model.data.day %>% 
  dplyr::select("STRIKE","WANGLE","WSPEED","TEMP","CLOUDH","COVER","COVERH","BIRDCOUNT","FLIGHTCOUNT","SLP","VIS","PRECIP","MONTH","DAY","WEEKOFYEAR")
  

head(model.data.day)
```


```{r Random Forest Variable Importance}
# Use Random Forest variable importance technique for variable selection
# The below list has been tailored after multiple iterations
fit <- randomForest::randomForest(
  as.factor(STRIKE) ~ WSPEED + WANGLE + COVER + BIRDCOUNT + FLIGHTCOUNT + MONTH + TEMP + COVERH + CLOUDH + YEAR + VIS + SLP + PRECIP + DAY,
  data = model.data.day,
  importance = TRUE,
  proximity = TRUE
)


varImp(fit)
randomForest::varImpPlot(fit, type = 2)
importanceOrder = order(-fit$importance)

names <- rownames(fit$importance)
names



# fit <- train(
#   as.factor(STRIKE) ~ .,
#   data = model.data.day,
#   method = "rf",
#   trControl = trainControl(
#     method = "cv",
#     number = 5,
#     allowParallel = TRUE,
#     verbose = TRUE
#   ),
#   # tuneGrid = expand.grid(mtry = c(4, 5, 6)),
#   importance = 'impurity'
# )

varImp(fit)


str(fit)
```


```{r Baruta}

# https://www.analyticsvidhya.com/blog/2016/03/select-important-variables-boruta-package/

var.boruta <-
  Boruta(
    as.factor(STRIKE) ~ WANGLE + WSPEED + TEMP + COVER + BIRDCOUNT + FLIGHTCOUNT + SLP + VIS + PRECIP + MONTH,
    data = model.data.day,
    doTrace = 2
    )

# plot(var.boruta, cex.axis = 0.8, las = 1)

lz <- lapply(1:ncol(var.boruta$ImpHistory), function(i)
  var.boruta$ImpHistory[is.finite(var.boruta$ImpHistory[, i]), i])
names(lz) <- colnames(var.boruta$ImpHistory)
Labels <- sort(sapply(lz, median))
plot(
  var.boruta,
  side = 1,
  las = 2,
  labels = names(Labels),
  at = 1:ncol(var.boruta$ImpHistory),
  cex.axis = 0.7
)
final.boruta <- TentativeRoughFix(var.boruta)
getSelectedAttributes(final.boruta, withTentative = F)
boruta.df <- attStats(final.boruta)
print(boruta.df)

```


```{r Train Control & Importance}
# prepare training scheme
control <- trainControl(method = "cv", number = 2)

# train the model
model <-
  train(
    as.factor(STRIKE) ~ WSPEED + WANGLE + COVER + BIRDCOUNT + FLIGHTCOUNT + MONTH + TEMP + COVERH + CLOUDH + YEAR + VIS + SLP + PRECIP + DAY,
    data = model.data.day,
    method = "lvq",
    preProcess = "scale",
    trControl = control
  )

# estimate variable importance
importance <- varImp(model, useModel = FALSE)

# summarize importance
print(importance)

# plot importance
plot(importance)
```

```{r}
library(h2o)
h2o.init()

# Identify predictors and response
y <- "STRIKE"
x <- setdiff(names(model.data.day), c("STRIKE", "STRIKECOUNT", "COVERH", "SLP", "DAY", "WEEKOFYEAR", "CLOUDH"))

# For binary classification, response should be a factor
model.data.day[, y] <- as.factor(model.data.day[, y])

# Run AutoML for 10 models
aml <- h2o.automl(x = x, y = y,
                  training_frame = as.h2o(model.data.day),
                  max_models = 10,
                  seed = 123)

# View the AutoML Leaderboard
lb <- aml@leaderboard
print(lb, n = nrow(lb))

# Get model ids for all models in the AutoML Leaderboard
model_ids <- as.data.frame(lb$model_id)[,1]

# View variable importance for all the models (besides Stacked Ensemble)
for (model_id in model_ids) {
  print(model_id)
  m <- h2o.getModel(model_id)
  h2o.varimp(m)
  h2o.varimp_plot(m)
}
```



```{r nzv}
# strikes.cat <-
#   model.data[, c("RTIME", "MONTH", "WEEKOFYEAR", "STRIKE")]
# 
# ### Ranking variables using penalized IV
# info.val.data <-
#   create_infotables(data = strikes.cat, y = as.numeric(strikes.cat$STRIKE), parallel = TRUE)
# 
# info.val = data.frame(info.val.data$Summary)
# info.val.data$Summary

nzv <- nearZeroVar(strikes.data.day, saveMetrics = TRUE)
nzv[nzv[,"zeroVar"] > 0, ] # Check for zero variance predictors. None
nzv[nzv[,"zeroVar"] + nzv[,"nzv"] > 0, ] # Check for near-zero variance predictors
```


```{r SMOTE over}
# SMOTE oversampling for Classification 
upsample.data <-
  SMOTE(
    STRIKE ~  WSPEED + TEMP + BIRDCOUNT + FLIGHTCOUNT + COVER  + MONTH,
    strikes.data.day,
    perc.over = 2000,
    k = 10
  )

upsample.data <- strikes.data.day %>% dplyr::select(STRIKE,
                                                 WSPEED,
                                                 TEMP,
                                                 BIRDCOUNT,
                                                 FLIGHTCOUNT,
                                                 COVER,
                                                 #RTIME,
                                                 WEEKOFYEAR,
                                                 MONTH)

upsample.data <- upsample.data %>%
  mutate(STRIKE = ifelse(STRIKE == 0, "NO", "YES"))

describe(upsample.data)

```

```{r downsample}
downsample.data <-
  SMOTE(
    STRIKE ~ WSPEED + TEMP + BIRDCOUNT + FLIGHTCOUNT + COVER + MONTH,
    strikes.data.day, #confirm with krishna to see if this should be strieks or model
    perc.under = 2900,
    k = 10
  )

downsample.data <- strikes.data.day %>% dplyr::select(STRIKE,
                                                 WSPEED,
                                                 TEMP,
                                                 BIRDCOUNT,
                                                 FLIGHTCOUNT,
                                                 COVER,
                                                 #RTIME,
                                                 WEEKOFYEAR,
                                                 MONTH)

downsample.data <- downsample.data %>%
  mutate(STRIKE = ifelse(STRIKE == 0, "NO", "YES"))

describe(downsample.data)

```

```{r create set up}
# Create the training and test datasets
set.seed(100)

model.data.day$STRIKE <- as.factor(model.data.day$STRIKE)

# Step 1: Get row numbers for the training data
trainRowNumbers.cl <-
  createDataPartition(model.data.day$STRIKE, p = 0.8, list = FALSE)

# Step 2: Create the training  dataset
train.cl.data <- model.data.day[trainRowNumbers.cl, ]

# Step 3: Create the test dataset
test.cl.data <- model.data.day[-trainRowNumbers.cl, ]

```

```{r create set up}
# Create the training and test datasets
set.seed(100)

# Step 1: Get row numbers for the training data
trainRowNumbers.up <-
  createDataPartition(upsample.data$STRIKE, p = 0.8, list = FALSE)

# Step 2: Create the training  dataset
train.up.data <- upsample.data[trainRowNumbers.up, ]

# Step 3: Create the test dataset
test.up.data <- upsample.data[-trainRowNumbers.up, ]

```

```{r create set down}
# Create the training and test datasets
set.seed(1234)

# Step 1: Get row numbers for the training data
trainRowNumbers.dn <-
  createDataPartition(downsample.data$STRIKE, p = 0.8, list = FALSE)

# Step 2: Create the training  dataset
train.dn.data <- downsample.data[trainRowNumbers.dn, ]

# Step 3: Create the test dataset
test.dn.data <- downsample.data[-trainRowNumbers.dn, ]

```


```{r 5 fold cv}

# 5 Fold cross validation with Probabilities
tc <- trainControl(
  method = "cv",
  number = 5,
  savePredictions = "final",
  classProbs = TRUE,
  verboseIter = TRUE,
  summaryFunction = twoClassSummary
)
```


```{r NB, up message=FALSE, warning=FALSE}

# Fit Naive Bayes Model for upsampled data
model.nb.up <- runModel(train.up.data, "nb")

validateAndPrintResult(model.nb.up, test.up.data)

```


```{r np down, message=FALSE, warning=FALSE}
# Fit Naive Bayes Model with downsampled data
model.nb.down <- runModel(train.dn.data, "nb")

validateAndPrintResult(model.nb.down, test.dn.data)

```

```{r Random Forest}
# Fit Random Forest Model with upsampled data
model.up.rf <- runModel(train.up.data, "rf")

validateAndPrintResult(model.up.rf, test.up.data)
```

```{r TP }
# Fit Random Forest Model with down sampled data
model.dn.rf <- runModel(train.dn.data, "rf")

validateAndPrintResult(model.dn.rf, test.dn.data)
```




```{r}
# Compare model performances using resample()
models_compare <-
  resamples(
    list(
      NB_UP = model.nb.up,
      # NB_DN = model.nb.down,
      RF_UP = model.up.rf
      # RF_DOWN = model.dn.rf
      # SVM_UP = model.up.svm,
      # SVM_DN = model.dn.svm
    )
  )

# Summary of the models performances
summary(models_compare)
```

```{r}
# Draw box plots to compare models
scales <- list(x = list(relation = "free"),
               y = list(relation = "free"))
bwplot(models_compare, scales = 'free', layout = c(1,1))
```


```{r}
model <-
  glm (
    as.factor(STRIKE) ~  WSPEED + TEMP + COVER + FLIGHTCOUNT + MONTH,
    data = train.data,
    family = binomial
  )
summary(model)
```

```{r}
## Predict the Values
predict <- predict(model, test.up.data, type = 'response')

## Create Confusion Matrix
table(test.up.data$STRIKE, predict > 0.009)

# ROC Curve
ROCRpred <- prediction(predict, test.up.data$STRIKE)
ROCRperf <- performance(ROCRpred, 'tpr','fpr')
plot(ROCRperf, colorize = TRUE, text.adj = c(-0.2,1.7))
ROCRperf

# # Predict using test data and generate confusion matrix
# predicted.response <- predict(model, test.up.data, )
# confusionMatrix(data = as.factor(predicted.response), reference = test.up.data$STRIKE)
# 
# predicted.response
summary(model) # Summary of model
cmLR <- table(predict>0.01, test.up.data$STRIKE)
cmLR # Confusion matrix
errorLR <- 100*(1-sum(diag(cmLR))/sum(cmLR))
errorLR # error rate
accuracyLR <- 100 - errorLR
accuracyLR # accuracy rate

#Code doesn't work-- check with Krishna
# precisionLR <- 100*cmLR[2,2]/sum(cmLR[2,1],cmLR[2,2]) 
# precisionLR # precision
# recallLR <- 100*cmLR[2,2]/sum(cmLR[1,2],cmLR[2,2]) 
# recallLR # recall
# FscoreLR <- 2*precisionLR*recallLR/(precisionLR+recallLR)
# FscoreLR # F-score
```

```{r}

train.data.factors <-
  model.matrix(
    as.factor(train.data$STRIKE) ~ train.data$MONTH + train.data$YEAR
  )[, -1]

model.train.data       <-
  as.matrix(
    data.frame(
      train.data$WSPEED,
      train.data$WANGLE,
      train.data$COVER,
      train.data$BIRDCOUNT,
      train.data$FLIGHTCOUNT,
      train.data$TEMP,
      train.data$PRECIP,
      train.data$VIS,
      train.data$SLP,
      train.data$COVERH,
      train.data$CLOUDH,
      train.data.factors
    )
  )

test.data.factors <-
  model.matrix(
    as.factor(test.data$STRIKE) ~ test.data$MONTH + test.data$YEAR
  )[, -1]

model.test.data       <-
  as.matrix(
    data.frame(
      test.data$WSPEED,
      test.data$WANGLE,
      test.data$COVER,
      test.data$BIRDCOUNT,
      test.data$FLIGHTCOUNT,
      test.data$TEMP,
      test.data$PRECIP,
      test.data$VIS,
      test.data$SLP,
      test.data$COVERH,
      test.data$CLOUDH,
      test.data.factors
    )
  )

# # Note alpha=1 for lasso only and can blend with ridge penalty down to
# # alpha=0 ridge only.
glmmod <- glmnet( model.train.data,
      as.factor(train.data$STRIKE),, alpha=1, family="binomial")

# Plot variable coefficients vs. shrinkage parameter lambda.
plot(glmmod, label=TRUE)
coef(glmmod)
print(glmmod)

# ELASTIC NET WITH 0 < ALPHA < 1
a <- seq(0.1, 0.9, 0.05)
search <- foreach(i = a, .combine = rbind) %dopar% {
  cv <-
    cv.glmnet(
      model.train.data,
      as.factor(train.data$STRIKE),
      family = "binomial",
      nfold = 10,
      type.measure = "deviance",
      paralle = TRUE,
      alpha = i
    )
  data.frame(
    cvm = cv$cvm[cv$lambda == cv$lambda.1se],
    lambda.1se = cv$lambda.1se,
    alpha = i
  )
}
plot(search$lambda.1se)
cv3 <- search[search$cvm == min(search$cvm), ]
md3 <-
  glmnet(
    model.train.data,
    as.factor(train.data$STRIKE),
    family = "binomial",
    lambda = cv3$lambda.1se,
    alpha = cv3$alpha
  )
coef(md3)

roc(as.factor(test.data$STRIKE), predict(md3, model.test.data, type = "response"))

preds <- predict(md3, model.test.data, type = "response")

# Calculate true positive rate and false positive rate on the prediction object
perf <- performance(prediction(preds, test.data$STRIKE), 'tpr', 'fpr')

plot(perf)

```

### Advanced Modeling

```{r}
models.reg <- c( "glm", "lm","glmnet", "glm.nb", "glmboost", "bayesglm")

# register parallel front-end
cl.reg <- makeCluster(4)

registerDoParallel(cl.reg)

# use lapply/loop to run everything
reg.train <- lapply(models.reg, function(i)
{
  cat("----------------------------------------------------",
      "\n")

  set.seed(123)
  cat(i, " <- loaded\n")

  tr <-
    train(
      reg.formula.best,
      train.data,
      method = i,
      trControl = trainControl(
        method = "cv",
        number = 5,
        verboseIter = TRUE,
        allowParallel = TRUE
      )
    )
})


r2 <- lapply(1:length(reg.train), function(i)
{
  cat(sprintf("%-20s", (models.reg[i])))

  cat(round(reg.train[[i]]$results$Rsquared[which.min(reg.train[[i]]$results$RMSE)], 4), "\t")

  cat(round(reg.train[[i]]$results$RMSE[which.min(reg.train[[i]]$results$RMSE)], 4), "\t")
  cat(reg.train[[i]]$times$everything[3], "\n")
})

# stop cluster and register sequntial front end
stopCluster(cl.reg)

registerDoSEQ()


# preallocate data types
i = 1
MAX = length(reg.train)

Name <- character() # Name
R2 <- numeric()   # R2
RMSE <- numeric()   # RMSE
MAE  <- numeric()   # MAE
Time <- numeric()   # time [s]
ModelName <- character() # long model name

# fill data and check indexes and NA
for (i in 1:length(reg.train)) {
  Name[i] <- reg.train[[i]]$method
  R2[i] <-
    as.numeric(reg.train[[i]]$results$Rsquared[which.min(reg.train[[i]]$results$RMSE)])
  RMSE[i] <-
    as.numeric(reg.train[[i]]$results$RMSE[which.min(reg.train[[i]]$results$RMSE)])
  MAE[i] <-
    as.numeric(reg.train[[i]]$results$MAE[which.min(reg.train[[i]]$results$MAE)])
  Time[i] <- as.numeric(reg.train[[i]]$times$everything[3])
  ModelName[i] <- reg.train[[i]]$modelInfo$label
}

# coerce to data frame
results <- data.frame(Name, R2, RMSE, MAE, Time, ModelName, stringsAsFactors = FALSE)

# call web output with correct column names
datatable(
  results,
  options = list(columnDefs = list(list(
    className = 'dt-left', targets = c(5, 4, 3, 2, 1, 0)
  )),
  pageLength = MAX,
  order = list(list(2, 'desc'))),
  caption = paste('Regression results from caret models', Sys.time()),
  class = 'cell-border stripe'
)  %>%
  formatRound('R2', 5) %>%
  formatRound('RMSE', 5) %>%
  formatRound('MAE', 5) %>%
  formatRound('Time', 5) %>%
  formatStyle(
    2,
    background = styleColorBar(R2, 'steelblue'),
    backgroundSize = '100% 90%',
    backgroundRepeat = 'no-repeat',
    backgroundPosition = 'center'
  )
```

```{r}

# all others may have just failed and are not listed here
models.cla <- c("knn", "AdaBoost.M1", "rf")

# register parallel front-end
cl.cla <- makeCluster(detectCores())
registerDoParallel(cl.cla)

# this setup actually calls the caret::train function, in order to provide
# minimal error handling this type of construct is needed.
trainCall <- function(i)
{
  cat("----------------------------------------------------",
      "\n")
  
  set.seed(123)
  cat(i, " <- loaded\n")
  
  t2 <-
    train(
      train.cl.data[, c("WSPEED",
                        "TEMP",
                        "BIRDCOUNT",
                        "FLIGHTCOUNT",
                        "COVER",
                        "WEEKOFYEAR",
                        "MONTH")],
      train.cl.data[,c('STRIKE')],
      # data = train.cl.data,
      method = i,
      trControl = trainControl(
        method = "boot632",
        number = 5
        # allowParallel = TRUE,
        # verboseIter = TRUE
        
      )
      # maxdepth = 10,
      # nu = 0.1
      # iter = 50
      # preProcess="scale",
      # na.action = na.omit()
    )
}

# use lapply/loop to run everything, required for try/catch error function to work
t2 <- lapply(models.cla, trainCall)

#remove NULL values, we only allow succesful methods, provenance is deleted.
t2 <- t2[!sapply(t2, is.null)]

# this setup extracts the results with minimal error handling 
# TrainKappa can be sometimes zero, but Accuracy SD can be still available
printCall <- function(i)
{
  return(tryCatch({
    cat(sprintf("%-22s", (models.cla[i])))
    cat(round(getTrainPerf(t2[[i]])$TrainAccuracy, 4), "\t")
    cat(round(getTrainPerf(t2[[i]])$TrainKappa, 4), "\t")
    cat(t2[[i]]$times$everything[3], "\n")
  },
  error = function(e)
    NULL))
}
	
r2 <- lapply(1:length(t2), printCall)

# stop cluster and register sequntial front end
stopCluster(cl.cla)
registerDoSEQ()


# preallocate data types
i = 1; MAX = length(t2);
x1 <- character() # Name
x2 <- numeric()   # R2
x3 <- numeric()   # RMSE
x4 <- numeric()   # time [s]
x5 <- character() # long model name
 
# fill data and check indexes and NA with loop/lapply
for (i in 1:length(t2)) {
  x1[i] <- t2[[i]]$method
  x2[i] <-
    as.numeric(round(getTrainPerf(t2[[i]])$TrainAccuracy, 4))
  x3[i] <- as.numeric(round(getTrainPerf(t2[[i]])$TrainKappa, 4))
  x4[i] <- as.numeric(t2[[i]]$times$everything[3])
  x5[i] <- t2[[i]]$modelInfo$label
}
  
# coerce to data frame
df1 <- data.frame(x1, x2, x3, x4, x5, stringsAsFactors = FALSE)

# print all results to R-GUI
df1

# plot models, just as example
# ggplot(t2[[1]])
# ggplot(t2[[1]])

# call web output with correct column names
datatable(
  df1,
  options = list(
    columnDefs = list(list(
      className = 'dt-left', targets = c(0, 1, 2, 3, 4, 5)
    )),
    pageLength = MAX,
    order = list(list(2, 'desc'))
  ),
  colnames = c('Num', 'Name', 'Accuracy', 'Kappa', 'time [s]', 'Model name'),
  caption = paste('Classification results from caret models', Sys.time()),
  class = 'cell-border stripe'
)  %>%
  formatRound('x2', 3) %>%
  formatRound('x3', 3) %>%
  formatRound('x4', 3) %>%
  formatStyle(
    2,
    background = styleColorBar(x2, 'steelblue'),
    backgroundSize = '100% 90%',
    backgroundRepeat = 'no-repeat',
    backgroundPosition = 'center'
  )

# print confusion matrix example
caret::confusionMatrix(t2[[1]])
```

```{r}
# 
# 
# 
# # There are a few ways to assemble a list of models to stack toegether:
# # 1. Train individual models and put them in a list
# # 2. Train a grid of models
# # 3. Train several grids of models
# # Note: All base models must have the same cross-validation folds and
# # the cross-validated predicted values must be kept.
# 
# 
# # 1. Generate a 2-model ensemble (GBM + RF)
# 
# # Train & Cross-validate a GBM
# my_gbm <- h2o.gbm(x = x,
#                   y = y,
#                   training_frame = train.data,
#                   distribution = "bernoulli",
#                   ntrees = 10,
#                   max_depth = 3,
#                   min_rows = 2,
#                   learn_rate = 0.2,
#                   nfolds = nfolds,
#                   fold_assignment = "Modulo",
#                   keep_cross_validation_predictions = TRUE,
#                   seed = 1)
# 
# # Train & Cross-validate a RF
# my_rf <- h2o.randomForest(x = x,
#                           y = y,
#                           training_frame = train.data,
#                           ntrees = 50,
#                           nfolds = nfolds,
#                           fold_assignment = "Modulo",
#                           keep_cross_validation_predictions = TRUE,
#                           seed = 1)
# 
# # Train a stacked ensemble using the GBM and RF above
# ensemble <- h2o.stackedEnsemble(x = x,
#                                 y = y,
#                                 training_frame = train.data,
#                                 model_id = "my_ensemble_binomial",
#                                 base_models = list(my_gbm, my_rf))
# 
# # Eval ensemble performance on a test set
# perf <- h2o.performance(ensemble, newdata = test.data)
# 
# # Compare to base learner performance on the test set
# perf_gbm_test <- h2o.performance(my_gbm, newdata = test.data)
# perf_rf_test <- h2o.performance(my_rf, newdata = test.data)
# baselearner_best_auc_test <- max(h2o.auc(perf_gbm_test), h2o.auc(perf_rf_test))
# ensemble_auc_test <- h2o.auc(perf)
# print(sprintf("Best Base-learner Test AUC:  %s", baselearner_best_auc_test))
# print(sprintf("Ensemble Test AUC:  %s", ensemble_auc_test))
# 
# # Generate predictions on a test set (if neccessary)
# pred <- h2o.predict(ensemble, newdata = test.data)
# 
# str(pred)
```

