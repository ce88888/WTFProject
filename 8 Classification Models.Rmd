---
title: "8 Classification Models"
author: "Team Strikes"
date: "`r format(Sys.time(), '%d %B, %Y')`"
output:
  html_document:
  code_folding: hide
  highlight: pygment
  theme: united
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```


```{r Check installed packages, echo = FALSE, warning=FALSE, message=FALSE}
# Creating a vector of packages used within
packages <- c(
  'arm',
  'boot',
  'Boruta',
  'car',
  'caret',
  'caTools',
  'data.table',
  'DMwR',
  'dplyr',
  'e1071',
  'Information',
  'klaR',
  'leaps',
  'lubridate',
  'magrittr',
  'MASS',
  'mctest',
  'mlbench',
  'MLeval',
  'pastecs',
  'PerformanceAnalytics',
  'pROC',
  'proxy',
  'pscl',
  'psych',
  'ranger',
  'ROCR',
  'Rtsne',
  'scales',
  'splitstackshape',
  'tidyselect',
  'tidyverse',
  'varrank',
  'VIM',
  'zoo',
  'corrplot',
  'glmnet',
  'doParallel',
  'foreach',
  'broom',
  'DT',
  'h2o',
  'AppliedPredictiveModeling',
  'xgboost'
)

# Checking for package installations on the system and installing if not found
if (length(setdiff(packages, rownames(installed.packages()))) > 0) {
  install.packages(setdiff(packages, rownames(installed.packages())))  
}

# Including the packages for use
for(package in packages){
  library(package, character.only = TRUE)
}

# Initiate Parallel
registerDoParallel(cores = 4)

# Initiate h2o automl
h2o.init()

```

```{r Model Data by Day}

# Read Classification dataset from RDS
class.data <- readRDS("KDEN_Class_Data.RDS")

```


```{r Test Train Splits}

# one-hot-encoding categorical features
ohe_feats = c('MONTH', 'YEAR')

# Create dummies
dummies <- dummyVars(~ MONTH +  YEAR, data = class.data)

df.dummies <- as.data.frame(predict(dummies, newdata = class.data))

# Merge Dummies to data frame
class.data <-
  cbind(class.data[, -c(which(colnames(class.data) %in% ohe_feats))], df.dummies)

class.data <-
  subset(class.data, select = -c(YEAR.2013, YEAR.2019))

# Create the training and test datasets
set.seed(100)

class.data$STRIKE <- as.factor(class.data$STRIKE)

# Step 1: Get row numbers for the training data
trainRowNumbers.cl <-
  createDataPartition(class.data$STRIKE, p = 0.75, list = FALSE)

# Step 2: Create the training  dataset
train.data <- class.data[trainRowNumbers.cl, ]

# Step 3: Create the test dataset
test.data <- class.data[-trainRowNumbers.cl, ]
```

```{r Validate Functions}
validateAndPrintResult <- function(model, test.data) {
  # Summarise Results
  print(model)
  
  ## run MLeval
  res <- evalm(model)
  
  # Predict on testData
  predicted.resp <- predict(model, test.data)
  head(predicted.resp)
  
  # Compute the confusion matrix
  confusionMatrix(
    reference = as.factor(test.data$STRIKE),
    data = predicted.resp,
    mode = 'everything',
    positive = 'YES'
  )
}
```

```{r}
# 5 Fold cross validation with Probabilities
tc <- trainControl(
  method = "cv",
  number = 5,
  savePredictions = "final",
  classProbs = TRUE,
  verboseIter = TRUE,
  summaryFunction = twoClassSummary
)
```

```{r Elastic Net Classification Technique}

model.train.data <- as.matrix(train.data[, -2])

model.test.data <- as.matrix(test.data[, -2])

# # Note alpha=1 for lasso only and can blend with ridge penalty down to
# # alpha=0 ridge only.
glmmod <- glmnet( model.train.data,
      as.factor(train.data$STRIKE),, alpha=1, family="binomial")

# Plot variable coefficients vs. shrinkage parameter lambda.
plot(glmmod, label=TRUE)
coef(glmmod)
print(glmmod)

# ELASTIC NET WITH 0 < ALPHA < 1
a <- seq(0.1, 0.9, 0.05)
search <- foreach(i = a, .combine = rbind) %dopar% {
  cv <-
    cv.glmnet(
      model.train.data,
      as.factor(train.data$STRIKE),
      family = "binomial",
      nfold = 10,
      type.measure = "deviance",
      paralle = TRUE,
      alpha = i
    )
  data.frame(
    cvm = cv$cvm[cv$lambda == cv$lambda.1se],
    lambda.1se = cv$lambda.1se,
    alpha = i
  )
}
plot(search$lambda.1se)
cv3 <- search[search$cvm == min(search$cvm), ]
md3 <-
  glmnet(
    model.train.data,
    as.factor(train.data$STRIKE),
    family = "binomial",
    lambda = cv3$lambda.1se,
    alpha = cv3$alpha
  )
coef(md3)

roc(as.factor(test.data$STRIKE), predict(md3, model.test.data, type = "response"))

preds <- predict(md3, model.test.data, type = "response")

# Calculate true positive rate and false positive rate on the prediction object
perf <- performance(prediction(preds, test.data$STRIKE), 'tpr', 'fpr')

plot(perf)
```

```{r}
model <-
  glm (
    STRIKE ~ WANGLE + TEMP + COVER + BIRDCOUNT + FLIGHTCOUNT + VIS, 
    data = train.data,
    family = binomial
  )
summary(model)

## Predict the Values
predict <- predict(model, test.data, type = 'response')

## Create Confusion Matrix
table(test.data$STRIKE, predict > 0.2)

# ROC Curve
ROCRpred <- prediction(predict, test.data$STRIKE)
ROCRperf <- performance(ROCRpred, 'tpr','fpr')
plot(ROCRperf, colorize = TRUE, text.adj = c(-0.2,1.7))
ROCRperf

# # Predict using test data and generate confusion matrix
# predicted.response <- predict(model, test.up.data, )
# confusionMatrix(data = as.factor(predicted.response), reference = test.up.data$STRIKE)
# 
# predicted.response
summary(model) # Summary of model
cmLR <- table(predict>0.2, test.data$STRIKE)
cmLR # Confusion matrix
errorLR <- 100*(1-sum(diag(cmLR))/sum(cmLR))
errorLR # error rate
accuracyLR <- 100 - errorLR
accuracyLR # accuracy rate

#Code doesn't work-- check with Krishna
precisionLR <- 100*cmLR[2,2]/sum(cmLR[2,1],cmLR[2,2])
precisionLR # precision
recallLR <- 100*cmLR[2,2]/sum(cmLR[1,2],cmLR[2,2])
recallLR # recall
FscoreLR <- 2*precisionLR*recallLR/(precisionLR+recallLR)
FscoreLR # F-score
```


```{r}
# all others may have just failed and are not listed here
models.cla <- c("knn", "AdaBoost.M1", "rf", )

# register parallel front-end
cl.cla <- makeCluster(detectCores())
registerDoParallel(cl.cla)

# this setup actually calls the caret::train function, in order to provide
# minimal error handling this type of construct is needed.
trainCall <- function(i)
{
  cat("----------------------------------------------------",
      "\n")
  
  set.seed(123)
  cat(i, " <- loaded\n")
  
  t2 <-
    train(
      train.data[, -2],
      train.data[, c('STRIKE')],
      method = i,
      trControl = trainControl(method = "boot632",
                               number = 5)
    )
}

# use lapply/loop to run everything, required for try/catch error function to work
t2 <- lapply(models.cla, trainCall)

#remove NULL values, we only allow succesful methods, provenance is deleted.
t2 <- t2[!sapply(t2, is.null)]

# this setup extracts the results with minimal error handling 
# TrainKappa can be sometimes zero, but Accuracy SD can be still available
printCall <- function(i)
{
  return(tryCatch({
    cat(sprintf("%-22s", (models.cla[i])))
    cat(round(getTrainPerf(t2[[i]])$TrainAccuracy, 4), "\t")
    cat(round(getTrainPerf(t2[[i]])$TrainKappa, 4), "\t")
    cat(t2[[i]]$times$everything[3], "\n")
  },
  error = function(e)
    NULL))
}
	
r2 <- lapply(1:length(t2), printCall)

# stop cluster and register sequntial front end
stopCluster(cl.cla)
registerDoSEQ()


# preallocate data types
i = 1; MAX = length(t2);
x1 <- character() # Name
x2 <- numeric()   # R2
x3 <- numeric()   # RMSE
x4 <- numeric()   # time [s]
x5 <- character() # long model name
 
# fill data and check indexes and NA with loop/lapply
for (i in 1:length(t2)) {
  x1[i] <- t2[[i]]$method
  x2[i] <-
    as.numeric(round(getTrainPerf(t2[[i]])$TrainAccuracy, 4))
  x3[i] <- as.numeric(round(getTrainPerf(t2[[i]])$TrainKappa, 4))
  x4[i] <- as.numeric(t2[[i]]$times$everything[3])
  x5[i] <- t2[[i]]$modelInfo$label
}
  
# coerce to data frame
df1 <- data.frame(x1, x2, x3, x4, x5, stringsAsFactors = FALSE)

# print all results to R-GUI
df1

# plot models, just as example
# ggplot(t2[[1]])
# ggplot(t2[[1]])

# call web output with correct column names
datatable(
  df1,
  options = list(
    columnDefs = list(list(
      className = 'dt-left', targets = c(0, 1, 2, 3, 4, 5)
    )),
    pageLength = MAX,
    order = list(list(2, 'desc'))
  ),
  colnames = c('Num', 'Name', 'Accuracy', 'Kappa', 'time [s]', 'Model name'),
  caption = paste('Classification results from caret models', Sys.time()),
  class = 'cell-border stripe'
)  %>%
  formatRound('x2', 3) %>%
  formatRound('x3', 3) %>%
  formatRound('x4', 3) %>%
  formatStyle(
    2,
    background = styleColorBar(x2, 'steelblue'),
    backgroundSize = '100% 90%',
    backgroundRepeat = 'no-repeat',
    backgroundPosition = 'center'
  )

# print confusion matrix example
caret::confusionMatrix(t2[[1]])
```


```{r}

# XGBoost ####

modelXGB_sample <- xgboost(
  data = as.matrix(train.data[, -2]),
  label = as.matrix(as.factor(ifelse(
    train.data$STRIKE == "NO", 0, 1
  ))),
  nrounds = 50,
  # optimal is 97
  max_depth = 50,
  # maximum depth of tree
  eta = 0.3,
  # step size shrinkage, learning rate
  nthread = 4,
  # number of threads to be used. 16 cores available
  "gamma" = 0,
  # minimum loss reduction, controls regularisation
  objective = "binary:logistic",
  min_child_weight = 1,
  # minimum number of instances required in a child node
  subsample = 1,
  # controls number of samples supplied to a tree
  colsample_bytree = 1,
  # controls number of features supplied to a tree
  save_period = NULL
) # controls number of features supplied to a tree


prob_predXGB_sample <- predict(modelXGB_sample, newdata = as.matrix(test.data[,-2])) # Predict the Test set results (probabilities)
predictXGB_sample = ifelse(prob_predXGB_sample > 0.5, 1, 0) # convert probabilities to binary

cmXGB_sample <- table(predictXGB_sample>0.7, test.data$STRIKE)
cmXGB_sample # Confusion matrix
errorXGB_sample <- 100*(1-sum(diag(cmXGB_sample))/sum(cmXGB_sample))
errorXGB_sample # error rate
accuracyXGB_sample <- 100 - errorXGB_sample
accuracyXGB_sample # accuracy rate
precisionXGB_sample <- 100*cmXGB_sample[2,2]/sum(cmXGB_sample[2,1],cmXGB_sample[2,2]) 
precisionXGB_sample # precision
recallXGB_sample <- 100*cmXGB_sample[2,2]/sum(cmXGB_sample[1,2],cmXGB_sample[2,2]) 
recallXGB_sample # recall
FscoreXGB_sample <- 2*precisionXGB_sample*recallXGB_sample/(precisionXGB_sample+recallXGB_sample) 
FscoreXGB_sample # F-score


# xgb.pred <-
#   prediction(prob_predXGB_sample, test.data) 

# xgb.perf <-
#   performance(xgb.pred, "tpr", "fpr")
# plot(
#   xgb.perf,
#   avg = "threshold",
#   colorize = TRUE,
#   lwd = 1,
#   main = "ROC Curve w/ Thresholds",
#   print.cutoffs.at = seq(0, 1, by = 0.05),
#   text.adj = c(-0.5, 0.5),
#   text.cex = 0.1
# )
# grid(col = "lightgray")
# axis(1, at = seq(0, 1, by = 0.1))
# axis(2, at = seq(0, 1, by = 0.1))
# abline(v = c(0.1, 0.3, 0.5, 0.7, 0.9),
#        col = "lightgray",
#        lty = "dotted") abline(h = c(0.1, 0.3, 0.5, 0.7, 0.9),
#                               col = "lightgray",
#                               lty = "dotted") lines(
#                                 x = c(0, 1),
#                                 y = c(0, 1),
#                                 col = "black",
#                                 lty = "dotted"
#                               )

```

```{r}

h2o.data <- class.data

h2o.data <-
  subset(h2o.data, select = -c(YEAR, DAY, SLP))

# one-hot-encoding categorical features
ohe_feats = c('MONTH', 'SEASON')

# Create dummies
dummies <- dummyVars(~ MONTH + SEASON, data = h2o.data)

df.dummies <- as.data.frame(predict(dummies, newdata = h2o.data))

# Merge Dummies to data frame
h2o.data <-
  cbind(h2o.data[, -c(which(colnames(h2o.data) %in% ohe_feats))], df.dummies)

# h2o.data <-
#   subset(h2o.data, select = -c(YEAR.2013, YEAR.2019))


# Create the training and test datasets
set.seed(100)

h2o.data$STRIKE <- as.factor(h2o.data$STRIKE)

# Step 1: Get row numbers for the training data
trainRowNumbers.cl <-
  createDataPartition(h2o.data$STRIKE, p = 0.75, list = FALSE)

# Step 2: Create the training  dataset
train.data <- h2o.data[trainRowNumbers.cl, ]

# Step 3: Create the test dataset
test.data <- h2o.data[-trainRowNumbers.cl, ]

train.data <- as.h2o(train.data)
test.data <- as.h2o(test.data)

# Identify predictors and response
y <- "STRIKE"
x <- setdiff(names(h2o.data), c("STRIKE"))

# For binary classification, response should be a factor
train.data[,y] <- as.factor(train.data[,y])
test.data[,y] <- as.factor(test.data[,y])

# Number of CV folds (to generate level-one data for stacking)
nfolds <- 5
```

```{r}
# 2. Generate a random grid of models and stack them together

# Some XGboost/GBM /rf hyperparameters
hyper_params <- list(ntrees = seq(10, 1000, 1),
                     learn_rate = seq(0.0001, 0.2, 0.0001),
                     max_depth = seq(1, 20, 1),
                     sample_rate = seq(0.5, 1.0, 0.0001),
                     col_sample_rate = seq(0.2, 1.0, 0.0001))

search_criteria <- list(strategy = "RandomDiscrete",
                        max_models = 10)

grid.id <-  as.character(format(Sys.time(), "%S"))


# Train & Cross-validate a RF
rf_grid <- h2o.grid(algorithm = "drf",
                     grid_id = paste0("grid_binomial_rf_",grid.id),
                     x = x,
                     y = y,
                     training_frame = train.data,
                     seed = 100,
                     nfolds = nfolds,
                     ntrees = 2500,
                     fold_assignment = "Modulo",
                     keep_cross_validation_predictions = TRUE)


gbm_grid <- h2o.grid(algorithm = "gbm",
                     grid_id = paste0("grid_binomial_gbm_",grid.id),
                     x = x,
                     y = y,
                     training_frame = train.data,
                     # ntrees = seq(10, 1000, 1),
                     seed = 100,
                     nfolds = nfolds,
                     fold_assignment = "Modulo",
                     keep_cross_validation_predictions = TRUE,
                     hyper_params = hyper_params,
                     search_criteria = search_criteria)



# Train the grid
xgb_grid <- h2o.grid(algorithm = "xgboost",
                     grid_id = paste0("grid_binomial_xgb_",grid.id),
                     x = x, 
                     y = y,
                     training_frame = train.data,
                     nfolds = nfolds,
                     seed = 100,
                     fold_assignment = "Modulo",
                     keep_cross_validation_predictions = TRUE,
                     hyper_params = hyper_params,
                     search_criteria = search_criteria)

# Train a stacked ensemble using the H2O and XGBoost models from above
base.models <- append(gbm_grid@model_ids,
                      xgb_grid@model_ids)

# Train a stacked ensemble using the GBM grid
ensemble <- h2o.stackedEnsemble(x = x,
                                y = y,
                                model_id = paste0("ensemble_gbm_grid_", grid.id, "_1"),
                                training_frame = train.data,
                                base_models = base.models)

# Eval ensemble performance on a test set
perf <- h2o.performance(ensemble, newdata = test.data)

# Compare to base learner performance on the test set
.getauc <-
  function(mm)
    h2o.auc(h2o.performance(h2o.getModel(mm), newdata = test.data))

baselearner_aucs <- sapply(base.models, .getauc)
baselearner_best_auc_test <- max(baselearner_aucs)
ensemble_auc_test <- h2o.auc(perf)
print(sprintf("Best Base-learner Test AUC:  %s", baselearner_best_auc_test))
print(sprintf("Ensemble Test AUC:  %s", ensemble_auc_test))

# Generate predictions on a test set (if neccessary)
pred <- h2o.predict(ensemble, newdata = test.data)

# Sort the grid by CV AUC for GBM
get_gbm_grid <- h2o.getGrid(grid_id = gbm_grid@grid_id, sort_by = "AUC", decreasing = TRUE)
get_gbm_grid
gbm_grid_top_model <- get_gbm_grid@summary_table[1, "model_ids"]
gbm_grid_top_model

# Sort the grid by CV AUC for XGBOOST
get_xgb_grid <- h2o.getGrid(grid_id = xgb_grid@grid_id, sort_by = "AUC", decreasing = TRUE)
get_xgb_grid
xgb_grid_top_model <- get_xgb_grid@summary_table[1, "model_ids"]
xgb_grid_top_model

# Sort the grid by CV AUC for XGBOOST
get_rf_grid <- h2o.getGrid(grid_id = rf_grid@grid_id, sort_by = "AUC", decreasing = TRUE)
get_rf_grid
rf_grid_top_model <- get_rf_grid@summary_table[1, "model_ids"]
rf_grid_top_model
```

```{r}
# Use AutoML to find a list of candidate models (i.e., leaderboard)
auto_ml <- h2o.automl(
  x = x,
  y = y,
  training_frame = train.data,
  nfolds = 5,
  max_runtime_secs = 60 * 120,
  max_models = 10,
  keep_cross_validation_predictions = FALSE,
  sort_metric = "auc",
  seed = 123,
  stopping_rounds = 50,
  stopping_metric = "auc",
  stopping_tolerance = 0
)

# Assess the leader board; the following truncates the results to show the top 
# and bottom 15 models. You can get the top model with auto_ml@leader
auto_ml@leaderboard %>% 
  as.data.frame() %>%
  dplyr::select(model_id, auc) %>%
  dplyr::slice(1:25)
```

