---
title: "8 Classification Models"
author: "Team Strikes"
date: "`r format(Sys.time(), '%d %B, %Y')`"
output:
  html_document:
    code_folding: hide
    highlight: pygment
    theme: united
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```


```{r Check installed packages, echo = FALSE, warning=FALSE, message=FALSE}
# Creating a vector of packages used within
packages <- c(
  'arm',
  'boot',
  'Boruta',
  'car',
  'caret',
  'caTools',
  'data.table',
  'DMwR',
  'dplyr',
  'e1071',
  'Information',
  'klaR',
  'leaps',
  'lubridate',
  'magrittr',
  'MASS',
  'mctest',
  'mlbench',
  'MLeval',
  'pastecs',
  'PerformanceAnalytics',
  'pROC',
  'proxy',
  'pscl',
  'psych',
  'ranger',
  'ROCR',
  'Rtsne',
  'scales',
  'splitstackshape',
  'tidyselect',
  'tidyverse',
  'varrank',
  'VIM',
  'zoo',
  'corrplot',
  'glmnet',
  'doParallel',
  'foreach',
  'broom',
  'DT',
  'h2o',
  'AppliedPredictiveModeling',
  'xgboost',
  'InformationValue',
  'caretEnsemble',
  'MLmetrics',
  'nnet'
)

# Checking for package installations on the system and installing if not found
if (length(setdiff(packages, rownames(installed.packages()))) > 0) {
  install.packages(setdiff(packages, rownames(installed.packages())))  
}

# Including the packages for use
for(package in packages){
  library(package, character.only = TRUE)
}

# Initiate Parallel
registerDoParallel(cores = 4)

# Initiate h2o automl
h2o.init()

```

```{r Model Data by Day}

# Read Classification dataset from RDS
class.data <- readRDS("KDEN_Class_Data_New.RDS")

# Reorder data frame by Date 
class.data <- class.data[order(class.data$DATE), ]

# class.data$PRCP_LAG_1 <- lag(class.data$PRCP, k = 1)

# class.data$TEMP_LAG_1 <- lag(class.data$TEMP, k = 1)

class.data <- class.data[complete.cases(class.data), ]

class.data <-
  class.data[!(class.data$MXSPD == 999.9 |
                 class.data$PRCP == 99.99), ] #| class.data$PRCP_LAG_1 == 99.99

class.data$FOG <- as.factor(class.data$FOG)
class.data$SNOW_ICE <- as.factor(class.data$SNOW_ICE)

class.data <- class.data %>%
  mutate(
    WEEK = lubridate::week(class.data$DATE),
    YEAR = lubridate::year(class.data$DATE)
  ) %>%
  mutate(RATIO = STRIKECOUNT / FLIGHTCOUNT * 10000)

t.data <- class.data %>%
  mutate(RATIO = STRIKECOUNT / FLIGHTCOUNT * 10000) %>%
  group_by(YEAR, WEEK) %>%
  summarise(RATIO = mean(RATIO)) %>%
  mutate(RISK = .bincode(
    RATIO,
    breaks = quantile(RATIO, probs = seq(0, 1, 1 / 3)),
    include.lowest = TRUE
  ) - 1) %>%
  dplyr::select(-RATIO)

class.data <-
  left_join(class.data, t.data, by = c("YEAR" = "YEAR", "WEEK" = "WEEK"))

class.data$RISK <-
  as.factor(ifelse(class.data$RISK == 0, "L", ifelse(class.data$RISK == 1, "M", "H")))

```


```{r Test Train Splits}

# one-hot-encoding categorical features
ohe_feats = c('MONTH')

# Create dummies
dummies <- dummyVars(~ MONTH, data = class.data)

df.dummies <- as.data.frame(predict(dummies, newdata = class.data))

# Merge Dummies to data frame
class.data <-
  cbind(class.data[, -c(which(colnames(class.data) %in% ohe_feats))], df.dummies)


valid.cl.data <-
  class.data[(class.data$YEAR == 2019), ]

class.data <-
  class.data %>% filter(!class.data$YEAR %in% c(1995:2007, 2019))

class.data <- subset(class.data, select = -c(DATE, YEAR, SEASON, MXSPD, SNOW_ICE, STRIKECOUNT, STRIKE, WEEK, RATIO, MONTH.12))

valid.cl.data <- subset(valid.cl.data, select = -c(DATE, YEAR, SEASON, MXSPD, SNOW_ICE, STRIKECOUNT, STRIKE, WEEK, RATIO, MONTH.12))

```

```{r}
# Create the training and test datasets
set.seed(100)

class.data$RISK <- as.factor(class.data$RISK)

# Step 1: Get row numbers for the training data
trainRowNumbers.cl <-
  createDataPartition(class.data$RISK, p = 0.70, list = FALSE)

# Step 2: Create the training  dataset
train.data <- class.data[trainRowNumbers.cl, ]

# Step 3: Create the test dataset
test.data <- class.data[-trainRowNumbers.cl, ]
```


```{r Validate Functions}

validateAndPrintResult <- function(model, data) {
  # Summarise Results
  print(model)
  
  ## run MLeval
  res <- evalm(model)
  
  ## get ROC
  
  res$roc
  
  ## get calibration curve
  
  res$cc
  
  ## get precision recall gain curve
  
  res$prg
  
  # Predict on testData
  predicted.resp <- predict(model, data)
  head(predicted.resp)
  
  caret::confusionMatrix(
    reference = as.factor(data$RISK),
    data = predicted.resp,
    mode = 'everything',
    positive = 'YES'
  )
}

```



```{r}

trControl <- trainControl(
  method = "cv",
  number = 7,
  savePredictions = "final",
  # index = createResample(as.factor(train.data$RISK), 7),
  classProbs = TRUE,
  summaryFunction = multiClassSummary
)

multinom.grid <- expand.grid(decay = 0)

xgbTreeGrid <-
  expand.grid(
    nrounds = 500,
    max_depth = seq(2, 8, by = 1),
    eta = 0.1,
    gamma = 0,
    colsample_bytree = 1.0,
    subsample = 1.0,
    min_child_weight = 4
  )

glmnetGridElastic <-
  expand.grid(.alpha = 0.3, .lambda = 0.009) ## notice the . before the parameter


gbm.tune.grid <-
  expand.grid(
    n.trees = c(400),
    interaction.depth = c(1, 3, 5),
    shrinkage = c(.01, .1, .3),
    n.minobsinnode = c(5, 10, 15)
  )


set.seed(333)

modelList <- caretList(
  RISK ~ .,
  
  train.data,
  
  trControl = trControl,
  
  metric = "logLoss",
  
  verbose = TRUE,
  
  tuneList = list(
    
    # Multinomial Logistic regression is using multinom method from nnet package
    multinom = caretModelSpec(method = 'multinom',
                              maxit = 150,
                              tuneGrid = multinom.grid), 
    
    ## Do not use custom names in list. Will give prediction error with greedy ensemble. Bug in caret.
    
    xgbTree = caretModelSpec(
      method = "xgbTree",
      tuneGrid = xgbTreeGrid,
      nthread = 8
    ),

    glmnet = caretModelSpec(method = "glmnet", tuneGrid = glmnetGridElastic),
    # Elastic, highly correlated with lasso and ridge regressions

    rf = caretModelSpec(
      method = "rf",
      ntree = 2000,
      tuneLength = 20,
      tuneGrid = data.frame(mtry = 10)
    ),
    # rf

    gbm = caretModelSpec(method = "gbm", tuneGrid = gbm.tune.grid)
    
  )
)


```

```{r}

validateAndPrintResult(modelList$multinom, test.data)

validateAndPrintResult(modelList$multinom, valid.cl.data)


```

```{r}

validateAndPrintResult(modelList$xgbTree, test.data)

validateAndPrintResult(modelList$xgbTree, valid.cl.data)


```

```{r}

validateAndPrintResult(modelList$glmnet, test.data)

validateAndPrintResult(modelList$glmnet, valid.cl.data)

```

```{r}


validateAndPrintResult(modelList$rf, test.data)

validateAndPrintResult(modelList$rf, valid.cl.data)
```

```{r}
validateAndPrintResult(modelList$gbm, test.data)

validateAndPrintResult(modelList$gbm, valid.cl.data)

```

```{r}

h2o.data <- class.data


# Create the training and test datasets
set.seed(100)

h2o.data$RISK <- as.factor(h2o.data$RISK)

# Step 1: Get row numbers for the training data
trainRowNumbers.cl <-
  createDataPartition(h2o.data$RISK, p = 0.75, list = FALSE)

# Step 2: Create the training  dataset
train.data <- h2o.data[trainRowNumbers.cl, ]

# Step 3: Create the test dataset
test.data <- h2o.data[-trainRowNumbers.cl, ]

train.data <- as.h2o(train.data)
test.data <- as.h2o(test.data)

# Identify predictors and response
y <- "RISK"
x <- setdiff(names(h2o.data), c("RISK"))

# For binary classification, response should be a factor
train.data[,y] <- as.factor(train.data[,y])
test.data[,y] <- as.factor(test.data[,y])

# Number of CV folds (to generate level-one data for stacking)
nfolds <- 5
```

```{r}
# # 2. Generate a random grid of models and stack them together
# 
# # Some XGboost/GBM /rf hyperparameters
# hyper_params <- list(
#   ntrees = seq(10, 1000, 1),
#   learn_rate = seq(0.0001, 0.2, 0.0001),
#   max_depth = seq(1, 20, 1),
#   sample_rate = seq(0.5, 1.0, 0.0001),
#   col_sample_rate = seq(0.2, 1.0, 0.0001)
# )
# 
# search_criteria <- list(strategy = "RandomDiscrete",
#                         max_models = 10)
# 
# grid.id <-  as.character(format(Sys.time(), "%S"))
# 
# 
# # Train & Cross-validate a RF
# rf_grid <- h2o.grid(
#   algorithm = "drf",
#   grid_id = paste0("grid_binomial_rf_", grid.id),
#   x = x,
#   y = y,
#   training_frame = train.data,
#   seed = 100,
#   nfolds = nfolds,
#   ntrees = 2500,
#   fold_assignment = "Modulo",
#   keep_cross_validation_predictions = TRUE
# )
# 
# 
# gbm_grid <- h2o.grid(
#   algorithm = "gbm",
#   grid_id = paste0("grid_binomial_gbm_", grid.id),
#   x = x,
#   y = y,
#   training_frame = train.data,
#   # ntrees = seq(10, 1000, 1),
#   seed = 100,
#   nfolds = nfolds,
#   fold_assignment = "Modulo",
#   keep_cross_validation_predictions = TRUE,
#   hyper_params = hyper_params,
#   search_criteria = search_criteria
# )
# 
# 
# 
# # Train the grid
# xgb_grid <- h2o.grid(
#   algorithm = "xgboost",
#   grid_id = paste0("grid_binomial_xgb_", grid.id),
#   x = x,
#   y = y,
#   training_frame = train.data,
#   nfolds = nfolds,
#   seed = 100,
#   fold_assignment = "Modulo",
#   keep_cross_validation_predictions = TRUE,
#   hyper_params = hyper_params,
#   search_criteria = search_criteria
# )
# 
# # Train a stacked ensemble using the H2O and XGBoost models from above
# base.models <- append(gbm_grid@model_ids,
#                       xgb_grid@model_ids)
# 
# # Train a stacked ensemble using the GBM grid
# ensemble <- h2o.stackedEnsemble(
#   x = x,
#   y = y,
#   model_id = paste0("ensemble_gbm_grid_", grid.id, "_24"),
#   training_frame = train.data,
#   base_models = base.models
# )
# 
# # Eval ensemble performance on a test set
# perf <- h2o.performance(ensemble, newdata = test.data)
# 
# # Compare to base learner performance on the test set
# .getmean_per_class_error <-
#   function(mm)
#     h2o.mean_per_class_error(h2o.performance(h2o.getModel(mm), newdata = test.data))
# 
# baselearner_aucs <- sapply(base.models, .getmean_per_class_error)
# baselearner_best_auc_test <- max(baselearner_aucs)
# ensemble_auc_test <- h2o.mean_per_class_error(perf)
# print(sprintf("Best Base-learner Test Mean per class error:  %s", baselearner_best_auc_test))
# print(sprintf("Ensemble Test Mean per class error:  %s", ensemble_auc_test))
# 
# # Generate predictions on a test set (if neccessary)
# pred <- h2o.predict(ensemble, newdata = test.data)
# 
# # Sort the grid by CV AUC for GBM
# get_gbm_grid <- h2o.getGrid(grid_id = gbm_grid@grid_id, sort_by = "mean_per_class_error", decreasing = TRUE)
# get_gbm_grid
# gbm_grid_top_model <- get_gbm_grid@summary_table[1, "model_ids"]
# gbm_grid_top_model
# 
# # Sort the grid by CV AUC for XGBOOST
# get_xgb_grid <- h2o.getGrid(grid_id = xgb_grid@grid_id, sort_by = "mean_per_class_error", decreasing = TRUE)
# get_xgb_grid
# xgb_grid_top_model <- get_xgb_grid@summary_table[1, "model_ids"]
# xgb_grid_top_model
# 
# # Sort the grid by CV AUC for XGBOOST
# get_rf_grid <- h2o.getGrid(grid_id = rf_grid@grid_id, sort_by = "mean_per_class_error", decreasing = TRUE)
# get_rf_grid
# rf_grid_top_model <- get_rf_grid@summary_table[1, "model_ids"]
# rf_grid_top_model
```

```{r}
# Use AutoML to find a list of candidate models (i.e., leaderboard)
auto_ml <- h2o.automl(
  x = x,
  y = y,
  training_frame = train.data,
  nfolds = 5,
  max_runtime_secs = 60 * 120,
  max_models = 10,
  keep_cross_validation_predictions = FALSE,
  sort_metric = "mean_per_class_error",
  seed = 123,
  stopping_rounds = 50,
  stopping_metric = "mean_per_class_error",
  stopping_tolerance = 0
)
auto_ml@leaderboard
# Assess the leader board; the following truncates the results to show the top 
# and bottom 15 models. You can get the top model with auto_ml@leader
auto_ml@leaderboard %>% 
  as.data.frame() %>%
  dplyr::select(model_id, mean_per_class_error) %>%
  dplyr::slice(1:25)
```

