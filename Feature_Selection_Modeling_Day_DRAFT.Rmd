---
title: "Count Model Workings"
author: "Tanu Kajla"
date: "January 25, 2020"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r Check installed packages, echo = FALSE, warning=FALSE, message=FALSE}
# Creating a vector of packages used within
packages <- c('arm',
'car',
'caret',
'caTools',
'data.table',
'DMwR',
'e1071',
'klaR',
'leaps',
'lubridate',
'magrittr',
'MASS',
'MLeval',
'PerformanceAnalytics',
'pROC',
'randomForest',
'ROCR',
'Rtsne',
'scales',
'tidyselect',
'tidyverse',
'VIM',
'zoo',
'Boruta',
'lares',
'mlbench',
'proxy',
'splitstackshape',
'varrank')

# Checking for package installations on the system and installing if not found
if (length(setdiff(packages, rownames(installed.packages()))) > 0) {
  install.packages(setdiff(packages, rownames(installed.packages())))  
}

# Including the packages for use
for(package in packages){
  library(package, character.only = TRUE)
}
library("ROCR", "lares", "caret","Information","genBart","limma")
require(pscl)
require(MASS)
require(boot)
#install.packages("countreg", repos="http://R-Forge.R-project.org")


```


```{r Model Data by Day}

########### Data Pre-Processing  ###########
# Read dataset from RDS
model.data <- readRDS("KDEN_FINAL.RDS")

#Running the Code to Group the Data by Day. THis removes Rtime, Arrivals, Departures
model.data.day <- model.data %>%
  group_by(`AIRPORT ID`,DATE) %>%
  summarise(`TOTAL` = sum(`TOTAL`), 
            STRIKES = sum(STRIKE), #Sum of Strikes
            STRIKE = sum(STRIKE), #Will be Converted to Binary Strikes
            BIRDCOUNT = first(BIRDCOUNT),
            TEMP = mean(TEMP,na.rm=TRUE), 
            SLP = mean(SLP, na.rm=TRUE),
            `WIND ANGLE` = mean(`WIND ANGLE`, na.rm=TRUE), 
            `WIND SPEED`= mean(`WIND SPEED`,na.rm=TRUE),
            `COVER`= mean(`COVER`,na.rm=TRUE),
            CLOUDH = mean(CLOUDH, na.rm=T), 
            COVERH = mean(COVERH, na.rm=T),
            PRECIP = mean(PRECIP, na.rm=T),
            VIS = mean(VIS, na.rm=T)) %>% 
    mutate(STRIKE = ifelse(STRIKE>0,1,0))

#Converting the new day data into a data frame 
model.data.day<-as.data.frame(model.data.day)
model.data.day$STRIKE<-as.factor(model.data.day$STRIKE)

# Add Year, Month, Day and Weekday fields to  the dataset as factors
date.add <- data.frame(
  YEAR = as.factor(format(model.data.day$DATE, format = "%Y")),
  MONTH = as.factor(format(model.data.day$DATE, format = "%m")),
  DAY = as.factor(format(model.data.day$DATE, format = "%d")),
  WEEKOFDAY = as.factor(format(model.data.day$DATE, format = "%V"))
)

# Bind new cols to the original dataset
model.data.day <- cbind(model.data.day, date.add)

# Remove AirportId and Date columns
model.data.day <- model.data.day[,-c(1,2)]

# Rename Windangle and windspeed
model.data.day <-
  model.data.day %>% dplyr::rename(WANGLE = `WIND ANGLE`,
                               WSPEED = `WIND SPEED`)

# Because of memory constraints considering only 2 years data
model.data.day <- model.data.day %>% filter(model.data.day$YEAR %in% c(2014,2015,2016,2017,2018))

########### Create the Training and Test Data Set Data Sets ###########
# Create the training and test datasets
set.seed(500)

# Step 1: Get row numbers for the training data
trainRowNumbers <-
  createDataPartition(model.data.day$STRIKE, p = 0.8, list = FALSE)

# Step 2: Create the training  dataset
train.data <- model.data.day[trainRowNumbers, ]

# Step 3: Create the test dataset
test.data <- model.data.day[-trainRowNumbers, ]

```


```{r Functions & Parameters}
########### Functions ###########
validateAndPrintResultC <- function(model, test.data) {
  # Summarise Results
  print(model)
  summary(model)
  model$finalModel

  # Predict on testData
  predicted.resp <- round(predict(model, test.data))
  
  # head(predicted.resp)
  compare<- test.data%>% 
  dplyr::select("YEAR","MONTH","DAY","STRIKES")
  
  compare<-cbind(compare,predicted.resp)
  compare$accuracy<- ifelse(compare$STRIKES == compare$predicted.resp,TRUE,FALSE)
  print(table(compare$accuracy))
  acc<- filter(compare, accuracy == TRUE, predicted.resp != 0) 
  false<- filter(compare, accuracy == FALSE)
  print(acc)
  print(false)

}

########### Parameters ###########
#train_control <- trainControl(method="repeatedcv", number=3, repeats=2)
train_control <- trainControl(method="cv", number=5)
#link ="log"

#total variables
regression<-STRIKES~
  TOTAL+
  STRIKE+
  BIRDCOUNT+
  TEMP+
  SLP+
  WANGLE+
  WSPEED+
  COVER+
  CLOUDH+
  COVERH+
  PRECIP+
  VIS+
  MONTH+
  DAY+
  WEEKOFDAY

#selected variables
regression1<-STRIKES~
  TOTAL+
  #STRIKE+
  BIRDCOUNT+
  TEMP+
  #SLP+
  WANGLE+
  WSPEED+
  #COVER+
  CLOUDH+
  COVERH+
  #PRECIP+
  VIS+
  #YEAR+
  MONTH+
  DAY+
  WEEKOFDAY

```

```{r}
corr.data <-
  model.data.day %>% dplyr::select(STRIKES,
                               WANGLE,
                               WSPEED,
                               TEMP,
                               CLOUDH,
                               COVER,
                               COVERH,
                               BIRDCOUNT,
                               TOTAL,
                               SLP,
                               VIS,
                               PRECIP)
corplot::corrplot(corr.data)


install.packages("corrplot")
source("http://www.sthda.com/upload/rquery_cormat.r")
cormat<-rquery.cormat(corr.data, graphType="heatmap")
item<-rquery.cormat(corr.data, type="flatten", graph=FALSE)
rquery.cormat(corr.data, type="full")
corr<-item$r
View(corr)
```


```{r Stepwise Feature Selection Model}
########### Stepwise Feature Selection (fully fitted)
#Run a Regular Stepwise Linar Regression on the Entire DataSet --- TAKES A LONG TIME WITH TIME VARIABLES
glmStepAIC <- train(regression,
                    data = model.data.day,
                    method="glmStepAIC",
                    trControl = train_control)
#Print the Results.. This is totally overfitted because it was run on the entire data set to get feature selection 
validateAndPrintResultC(glmStepAIC,test.data)
sink(type = "output")

########### RANDOM AIC RESULTS ########### 

# 729 samples
#  12 predictor
# 
# No pre-processing
# Resampling: Cross-Validated (5 fold) 
# Summary of sample sizes: 584, 583, 583, 583, 583 
# Resampling results:
# 
#   RMSE       Rsquared   MAE      
#   0.3121226  0.7730515  0.1370295
# 
# 
# FALSE  TRUE 
#    10   135 

#Altnerate modelbase
#modelbase<-glm(STRIKES~ TOTAL+ STRIKE+ BIRDCOUNT+ TEMP+ SLP+ WANGLE+ WSPEED+ COVER+ CLOUDH+ COVERH+ PRECIP+ VIS+ WEEKOFDAY, data = model.data.day)
#alternate feature selection 
#step <- stepAIC(modelbase)
#step$anova # display results

# define training control

# Step:  AIC=329.67
# .outcome ~ STRIKE + TEMP + WANGLE + WSPEED + COVER + CLOUDH + 
#     PRECIP

# Step:  AIC=377.81
# .outcome ~ STRIKE1 + TEMP + WANGLE + COVER + VIS
# 
#           Df Deviance     AIC
# <none>         70.306  377.81
# - VIS      1   70.568  378.53
# - COVER    1   70.606  378.92
# - WANGLE   1   70.650  379.37
# - TEMP     1   70.668  379.56
# - STRIKE1  1  291.892 1413.57
# Generalized Linear Model with Stepwise Feature Selection 


# Step:  AIC=377.81
# .outcome ~ STRIKE1 + TEMP + WANGLE + COVER + VIS
# 
#           Df Deviance     AIC
# <none>         70.306  377.81
# - VIS      1   70.568  378.53
# - COVER    1   70.606  378.92
# - WANGLE   1   70.650  379.37
# - TEMP     1   70.668  379.56
# - STRIKE1  1  291.892 1413.57
# Generalized Linear Model with Stepwise Feature Selection 

#After runing all the variables

# Step:  AIC=1361.23
# .outcome ~ WSPEED + VIS + MONTH08 + MONTH09 + MONTH10 + DAY03 + 
#     DAY07 + DAY29 + WEEKOFDAY19 + WEEKOFDAY20 + WEEKOFDAY21 + 
#     WEEKOFDAY22 + WEEKOFDAY23 + WEEKOFDAY24 + WEEKOFDAY25 + WEEKOFDAY26 + 
#     WEEKOFDAY27 + WEEKOFDAY29 + WEEKOFDAY30 + WEEKOFDAY31 + WEEKOFDAY32 + 
#     WEEKOFDAY33 + WEEKOFDAY34 + WEEKOFDAY35 + WEEKOFDAY37 + WEEKOFDAY40

```

```{Feature Selection}
# step <- stepAIC(model1)
# step$anova # display results
# plot(model1, cex.axis = 0.8, las = 1)

```


```{r Regression Model}
########### General Model ########### 
modelglm <- train(regression1,
                  data=train.data,
                  method="glm" ,
                  trControl = train_control)

validateAndPrintResultC(modelglm,test.data)
########### Results ########### 

# 584 samples
#  11 predictor
# 
# No pre-processing
# Resampling: Cross-Validated (5 fold) 
# Summary of sample sizes: 468, 468, 467, 467, 466 
# Resampling results:
# 
#   RMSE       Rsquared    MAE      
#   0.6766016  0.03640392  0.4935009
# 
# prediction from a rank-deficient fit may be misleading
# FALSE  TRUE 
#    54    91 

# 8 Accurate predicted Flights

```


```{r Negative Binomial}
#Negative Binomial
modelglm.nb <- glm.nb(regression1,
                  data=train.data, link = "sqrt")

validateAndPrintResultC(modelglm.nb,test.data)


##Updating the link to sqrt
# FALSE  TRUE 
#   207   156 
#countreg::rootogram(modelglm.nb)

########### Results ########### 

#The model is currently predicting negative strikes.. how do i control for this? 

# Degrees of Freedom: 583 Total (i.e. Null);  483 Residual
# Null Deviance:	    550.8 
# Residual Deviance: 383.6 	AIC: 922.8
# 
# FALSE  TRUE 
#   129    16 


## want to try the NEGBINOMAIL with CAROT -- DOESN"T WORK
# train_control <- trainControl(method="repeatedcv", number=3, repeats=2)
# 
# modelglm.nb <- train(STRIKES~
#                        TOTAL+
#                         #STRIKE+
#                         BIRDCOUNT+
#                         TEMP+
#                         #SLP+
#                         WANGLE+
#                         WSPEED+
#                         #COVER+
#                         CLOUDH+
#                         COVERH+
#                         #PRECIP+
#                         VIS,
#                         #YEAR+
#                         #MONTH+
#                         #DAY+
#                         #WEEKOFDAY,
#                   data=train.data,
#                   method="glm.nb" ,
#                   trControl = train_control,
#                   link="sqrt")
# validateAndPrintResultC(modelglm.nb,test.data)

```


```{r }
##Poisson
modelPos <- glm(regression1, 
               data = train.data, family= poisson(link = "sqrt"))

validateAndPrintResultC(modelPos,test.data)

########### Results ########### 

#Updating the link
# Degrees of Freedom: 1458 Total (i.e. Null);  1357 Residual
# Null Deviance:	    1464 
# Residual Deviance: 1236 	AIC: 2300
# 
# FALSE  TRUE 
#   208   155 



#The model is currently predicting negative strikes.. how do i control for this? 

# Degrees of Freedom: 583 Total (i.e. Null);  483 Residual
# Null Deviance:	    550.9 
# Residual Deviance: 383.6 	AIC: 920.8
# 
# FALSE  TRUE 
#   129    16 

```

```{r}
##QuasiPosson
modelQPos<-lm(regression1, 
               data = train.data, 
                family = quasipoisson(link = "sqrt"))
validateAndPrintResultC(modelQPos,test.data)

#Looks Promising 

```


```{r Information Gain}
model.hurdle<- pscl::hurdle(regression3, data = train.data, dist = "poisson", zero.dist = "binomial")
validateAndPrintResultC(model.hurdle,test.data)

regression3<-STRIKES~
  TOTAL+
  #STRIKE+
  #BIRDCOUNT
  #TEMP+
  #SLP+
  #WANGLE+
  #WSPEED+
  #COVER+
  #CLOUDH+
  #COVERH+
  #PRECIP+
  #VIS
  #YEAR+
  #MONTH+
  #DAY+
  #WEEKOFDAY

#Cant run the hurdle since the variables are highly correlated with one another
  
```


```{r}


```


```{r}


```

```{r}

```


```{r}

```


```{r}

```


```{r}

```


```{r}

```

