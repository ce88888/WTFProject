---
title: "Count Model Workings"
author: "Tanu Kajla"
date: "January 25, 2020"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r Check installed packages, echo = FALSE, warning=FALSE, message=FALSE}
# Creating a vector of packages used within
packages <- c('arm','boot',
'car',
'caret',
'caTools',
'data.table',
'DMwR',
'e1071',
'klaR',
'leaps',
'lubridate',
'magrittr',
'MASS',
'MLeval',
'PerformanceAnalytics',
'pROC',
'pscl',
'randomForest',
'ROCR',
'Rtsne',
'scales',
'tidyselect',
'tidyverse',
'VIM',
'zoo',
'Boruta',
'mlbench',
'proxy',
'splitstackshape',
'varrank',
'dplyr')

# Checking for package installations on the system and installing if not found
if (length(setdiff(packages, rownames(installed.packages()))) > 0) {
  install.packages(setdiff(packages, rownames(installed.packages())))  
}

# Including the packages for use
for(package in packages){
  library(package, character.only = TRUE)
}

#install.packages("countreg", repos="http://R-Forge.R-project.org")

```


```{r Model Data by Day}

########### Data Pre-Processing  ###########
# Read dataset from RDS
model.data <- readRDS("KDEN_FINAL.RDS")

#Running the Code to Group the Data by Day. THis removes Rtime, Arrivals, Departures
model.data.day <- model.data %>%
  group_by(`AIRPORT ID`,DATE) %>%
  summarise(`TOTAL` = sum(`TOTAL`), 
            STRIKES = sum(STRIKE), #Sum of Strikes
            STRIKE = sum(STRIKE), #Will be Converted to Binary Strikes
            BIRDCOUNT = first(BIRDCOUNT),
            TEMP = mean(TEMP,na.rm=TRUE), 
            SLP = mean(SLP, na.rm=TRUE),
            `WIND ANGLE` = mean(`WIND ANGLE`, na.rm=TRUE), 
            `WIND SPEED`= mean(`WIND SPEED`,na.rm=TRUE),
            `COVER`= mean(`COVER`,na.rm=TRUE),
            CLOUDH = mean(CLOUDH, na.rm=T), 
            COVERH = mean(COVERH, na.rm=T),
            PRECIP = mean(PRECIP, na.rm=T),
            VIS = mean(VIS, na.rm=T)) %>% 
    mutate(STRIKE = ifelse(STRIKE>0,1,0))

#Converting the new day data into a data frame 
model.data.day<-as.data.frame(model.data.day)
model.data.day$STRIKE<-as.factor(model.data.day$STRIKE)

# Add Year, Month, Day and Weekday fields to  the dataset as factors
date.add <- data.frame(
  YEAR = as.factor(format(model.data.day$DATE, format = "%Y")),
  MONTH = as.factor(format(model.data.day$DATE, format = "%m")),
  DAY = as.factor(format(model.data.day$DATE, format = "%d")),
  WEEKOFYEAR = as.factor(format(model.data.day$DATE, format = "%V"))
)

# Bind new cols to the original dataset
model.data.day <- cbind(model.data.day, date.add)

# Remove AirportId and Date columns
model.data.day <- model.data.day[,-c(1,2)]

# Rename Windangle and windspeed
model.data.day <-
  model.data.day %>% dplyr::rename(WANGLE = `WIND ANGLE`,
                               WSPEED = `WIND SPEED`)

# Because of memory constraints considering only 2 years data
model.data.day <- model.data.day %>% filter(model.data.day$YEAR %in% c(2014,2015,2016,2017,2018))

########### Create the Training and Test Data Set Data Sets ###########
# Create the training and test datasets
set.seed(500)

# Step 1: Get row numbers for the training data
trainRowNumbers <-
  createDataPartition(model.data.day$STRIKE, p = 0.8, list = FALSE)

# Step 2: Create the training  dataset
train.data <- model.data.day[trainRowNumbers, ]

# Step 3: Create the test dataset
test.data <- model.data.day[-trainRowNumbers, ]

```


```{r Functions & Parameters}
########### Functions ###########
validateAndPrintResultC <- function(model, test.data) {
  # Summarise Results
  print(model)
  summary(model)
  model$finalModel

  # Predict on testData
  predicted.resp <- round(predict(model, test.data))
  
  # head(predicted.resp)
  compare<- test.data%>% 
  dplyr::select("YEAR","MONTH","DAY","STRIKES")
  
  compare<-cbind(compare,predicted.resp)
  compare$accuracy<- ifelse(compare$STRIKES == compare$predicted.resp,TRUE,FALSE)
  acc<- filter(compare, accuracy == TRUE, predicted.resp != 0) 
  false<- filter(compare, accuracy == FALSE)
  print(table(compare$accuracy))
  print(acc)
  print(false)

}
source("http://www.sthda.com/upload/rquery_cormat.r")

########### Parameters ###########
#train_control <- trainControl(method="repeatedcv", number=3, repeats=2)
train_control <- trainControl(method="cv", number=5)

#all variables excluding the strikes factor (highly correlated, no value)
#total variables
regression<-STRIKES~
  TOTAL+
  #STRIKE+
  BIRDCOUNT+
  TEMP+
  SLP+
  WANGLE+
  WSPEED+
  COVER+
  CLOUDH+
  COVERH+
  PRECIP+
  VIS+
  MONTH+
  #DAY+
  WEEKOFYEAR

#selected variables
regression1<-STRIKES~
  TOTAL+
  #STRIKE+
  BIRDCOUNT+
  TEMP+
  #SLP+
  WANGLE+
  WSPEED+
  #COVER+
  CLOUDH+
  COVERH+
  #PRECIP+
  VIS+
  #YEAR+
  MONTH+
  DAY+
  WEEKOFYEAR

#removal of time variables
regression2<-STRIKES~
  TOTAL+
  #STRIKE+
  BIRDCOUNT+
  TEMP+
  SLP+
  WANGLE+
  WSPEED+
  COVER+
  CLOUDH+
  COVERH+
  PRECIP+
  VIS
  #MONTH+
  #DAY+
  #WEEKOFYEAR

```

```{r}
corr.data <-
  model.data.day %>% dplyr::select(STRIKES,
                               WANGLE,
                               WSPEED,
                               TEMP,
                               CLOUDH,
                               COVER,
                               COVERH,
                               BIRDCOUNT,
                               TOTAL,
                               SLP,
                               VIS,
                               PRECIP)
corr.list<-rquery.cormat(corr.data, type="flatten", graph=FALSE)
cormat<-rquery.cormat(corr.data, graphType="heatmap")
rquery.cormat(corr.data, type="full")
corr<-corr.list$r

```


```{r Stepwise Feature Selection Model}
########### Stepwise Feature Selection (fully fitted)
#Run a Regular Stepwise Linar Regression on the Entire DataSet --- TAKES A LONG TIME WITH TIME VARIABLES
glmStepAIC <- train(regression2,
                    data = model.data.day,
                    method="glmStepAIC",
                    trControl = train_control)
#Print the Results.. This is totally overfitted because it was run on the entire data set to get feature selection 
validateAndPrintResultC(glmStepAIC,test.data)

########### RANDOM AIC RESULTS ########### 

# 729 samples
#  12 predictor
# 
# No pre-processing
# Resampling: Cross-Validated (5 fold) 
# Summary of sample sizes: 584, 583, 583, 583, 583 
# Resampling results:
# 
#   RMSE       Rsquared   MAE      
#   0.3121226  0.7730515  0.1370295
# 
# 
# FALSE  TRUE 
#    10   135 

#Altnerate modelbase
#modelbase<-glm(STRIKES~ TOTAL+ STRIKE+ BIRDCOUNT+ TEMP+ SLP+ WANGLE+ WSPEED+ COVER+ CLOUDH+ COVERH+ PRECIP+ VIS+ WEEKOFYEAR, data = model.data.day)
#alternate feature selection 
#step <- stepAIC(modelbase)
#step$anova # display results

# define training control

# Step:  AIC=329.67
# .outcome ~ STRIKE + TEMP + WANGLE + WSPEED + COVER + CLOUDH + 
#     PRECIP

# Step:  AIC=377.81
# .outcome ~ STRIKE1 + TEMP + WANGLE + COVER + VIS
# 
#           Df Deviance     AIC
# <none>         70.306  377.81
# - VIS      1   70.568  378.53
# - COVER    1   70.606  378.92
# - WANGLE   1   70.650  379.37
# - TEMP     1   70.668  379.56
# - STRIKE1  1  291.892 1413.57
# Generalized Linear Model with Stepwise Feature Selection 

#After runing all the variables

# Step:  AIC=1361.23
# .outcome ~ WSPEED + VIS + MONTH08 + MONTH09 + MONTH10 + DAY03 + 
#     DAY07 + DAY29 + WEEKOFYEAR19 + WEEKOFYEAR20 + WEEKOFYEAR21 + 
#     WEEKOFYEAR22 + WEEKOFYEAR23 + WEEKOFYEAR24 + WEEKOFYEAR25 + WEEKOFYEAR26 + 
#     WEEKOFYEAR27 + WEEKOFYEAR29 + WEEKOFYEAR30 + WEEKOFYEAR31 + WEEKOFYEAR32 + 
#     WEEKOFYEAR33 + WEEKOFYEAR34 + WEEKOFYEAR35 + WEEKOFYEAR37 + WEEKOFYEAR40

#             Df Deviance    AIC
# <none>           801.92 3691.3
# - BIRDCOUNT  1   802.88 3691.5
# - TOTAL      1   803.52 3693.0
# - VIS        1   804.20 3694.5
# - SLP        1   804.28 3694.7
# - COVERH     1   806.00 3698.6
# - TEMP       1   815.75 3720.5
# Generalized Linear Model with Stepwise Feature Selection 
# 
# 1822 samples
#   11 predictor


```

```{r Regression Model}
########### General Model ########### 
modelglm <- train(regression1,
                  data=train.data,
                  method="glm" ,
                  trControl = train_control)

validateAndPrintResultC(modelglm,test.data)

plot(caret::varImp(modelglm))

########### Results ########### 

# 584 samples
#  11 predictor
# 
# No pre-processing
# Resampling: Cross-Validated (5 fold) 
# Summary of sample sizes: 468, 468, 467, 467, 466 
# Resampling results:
# 
#   RMSE       Rsquared    MAE      
#   0.6766016  0.03640392  0.4935009
# 
# prediction from a rank-deficient fit may be misleading
# FALSE  TRUE 
#    54    91 

# 8 Accurate predicted Flights

```


```{r Negative Binomial}
#Negative Binomial
modelglm.nb <- glm.nb(regression1,
                  data=train.data, link = "sqrt",
                  method = "glm.fit")

validateAndPrintResultC(modelglm.nb,test.data)

plot(modelglm.nb)

########### Results ########### 
# Degrees of Freedom: 1458 Total (i.e. Null);  1357 Residual
# Null Deviance:	    1318 
# Residual Deviance: 1109 	AIC: 2295
# 
# FALSE  TRUE 
#   207   156 
########### Failed Code ########### 

## want to try the NEGBINOMAIL with CAROT -- DOESN"T WORK
# train_control <- trainControl(method="repeatedcv", number=3, repeats=2)
# 
# modelglm.nb <- train(STRIKES~
#                        TOTAL+
#                         #STRIKE+
#                         BIRDCOUNT+
#                         TEMP+
#                         #SLP+
#                         WANGLE+
#                         WSPEED+
#                         #COVER+
#                         CLOUDH+
#                         COVERH+
#                         #PRECIP+
#                         VIS,
#                         #YEAR+
#                         #MONTH+
#                         #DAY+
#                         #WEEKOFYEAR,
#                   data=train.data,
#                   method="glm.nb" ,
#                   trControl = trainControl(method = "boot"),
#                   link="sqrt",
#                   trace =TRUE,
#                   maxit = 10)
# validateAndPrintResultC(modelglm.nb,test.data)

```


```{r }
##Poisson
modelPos <- glm(regression1, 
               data = train.data, 
               family= poisson(link = "sqrt"))

validateAndPrintResultC(modelPos,test.data)

plot(modelPos)

########### Results ########### 

#Updating the link
# Degrees of Freedom: 1458 Total (i.e. Null);  1357 Residual
# Null Deviance:	    1464 
# Residual Deviance: 1236 	AIC: 2300
# 
# FALSE  TRUE 
#   208   155 



#The model is currently predicting negative strikes.. how do i control for this? 

# Degrees of Freedom: 583 Total (i.e. Null);  483 Residual
# Null Deviance:	    550.9 
# Residual Deviance: 383.6 	AIC: 920.8
# 
# FALSE  TRUE 
#   129    16 

```

```{r}
##QuasiPosson
modelQPos<-glm(regression1, 
              data = train.data, quasipoisson)
              #family = quasipoisson(link = "sqrt"))
              
validateAndPrintResultC(modelQPos,test.data)

plot(modelQPos)

#Looks Promising 

```


```{r Information Gain}
model.hurdle<- pscl::hurdle(regression3, data = train.data, dist = "poisson", zero.dist = "binomial")
validateAndPrintResultC(model.hurdle,test.data)

regression3<-STRIKES~
  TOTAL+
  #STRIKE+
  #BIRDCOUNT
  #TEMP+
  #SLP+
  #WANGLE+
  #WSPEED+
  #COVER+
  #CLOUDH+
  #COVERH+
  #PRECIP+
  #VIS
  #YEAR+
  #MONTH+
  #DAY+
  #WEEKOFYEAR

#Cant run the hurdle since the variables are highly correlated with one another
  
```


```{r ols}
ols <-lm(regression1, data = train.data)
validateAndPrintResultC(ols,test.data)
plot(ols)

```


```{r Fixed Effects-- DOESN't WORK}
regression.time<-STRIKES~
   TOTAL+
  # #STRIKE+
   BIRDCOUNT+
  # TEMP+
  # #SLP+
  # WANGLE+
   WSPEED+
  # #COVER+
  # CLOUDH+
  # COVERH+
  # #PRECIP+
  # VIS+
  #YEAR+
  MONTH+
  factor(DAY)+
  factor(WEEKOFYEAR)

fixed.dum <-lm(regression.time, data = train.data)
validateAndPrintResultC(fixed.dum,test.data)

summary(fixed.dum)
plot(fixed.dum)
fixed.time<- plm::plm(regression.time, 
                      data = train.data, 
                      model = "within")

summary(fixed.time)
random.time <- plm::plm(regression.time, 
                  data=train.data, 
                  model="random")
# effect = c("individual",
#   "time", "twoways", "nested")
# model = c("within", "random", "ht",
#   "between", "pooling", "fd")

summary(fixed)
#doesn't explain anything and has a very low r^2

```

```{r}
# To decide between fixed or random effects you can run a Hausman test where the null hypothesis is that the preferred model is random effects vs. the alternative the fixed effects (see Green, 2008, chapter 9). It basically tests whether the unique errors are correlated with the regressors, the null hypothesis is they are not. If the p-value is significant (for example <0.05) then use fixed effects, if not use random effects.

phtest(fixed, random)

```


```{r}

```


```{r}

```


```{r}

```


```{r}

```

